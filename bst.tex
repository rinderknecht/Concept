\chapter{Arbres binaires de recherche}

La recherche d'un n{\oe}ud interne dans un arbre binaire peut être
couteuse parce que, dans le pire des cas, l'arbre en entier doit être
parcouru, par exemple, en préfixe ou en largeur. Pour remédier à cela,
deux choses sont désirables: l'arbre binaire devrait être aussi
équilibré\index{arbre binaire!$\sim$ équilibré} que possible et le
choix de visiter le sous-arbre gauche ou droit ne devrait être fait
qu'après avoir examiné le contenu de la racine, appelé
\emph{clé}\index{clé}. La solution la plus simple consiste à
satisfaire la dernière condition et voir si elle sied à la première.

Un \emph{arbre binaire de recherche} \citep{Mahmoud_1992}\index{arbre
  binaire de recherche}, noté \(\fun{bst}(x, t_1,
t_2)\),\index{bst@\fun{bst/3}}
\begin{wrapfigure}[7]{r}[0pt]{0pt}
\centering
\includegraphics[bb=68 661 123 721]{bst_ex}
\caption{\label{fig:bst_ex}}
\end{wrapfigure}
est un arbre binaire tel que la clé~\(x\) est supérieure aux clés
de~\(t_1\) et inférieure aux clés de~\(t_2\). (Le n{\oe}ud externe
\(\fun{ext}()\) est un arbre de recherche trivial.) La fonction de
comparaison dépend de la nature des clés, mais doit être
\emph{totale}, c'est-à-dire que toute clé est comparable avec
n'importe quelle autre clé. Un exemple est donné à la
\fig~\vref{fig:bst_ex}. Une conséquence immédiate est que le parcours
infixe\index{arbre binaire!parcours infixe} d'un arbre binaire de
recherche est toujours une pile triée par ordre croissant, par exemple
\([3,5,11,13,17,29]\) pour l'arbre de la \fig~\ref{fig:bst_ex}
ci-dessus.

Cette propriété permet de vérifier simplement qu'un arbre binaire est
en fait un arbre de recherche: effectuer un parcours infixe et ensuite
vérifier l'ordre de la pile résultante. La fonction correspondante,
\fun{bst\(_0\)/1}\index{bst0@\fun{bst\(_0\)/1}}, est lisible à la
\fig~\vref{fig:bst0},
\begin{figure}[t]
\begin{equation*}
\boxed{
\begin{array}{r@{\;}l@{\;}l}
\fun{bst}_0(t) & \rightarrow & \fun{ord}(\fun{in}_2(t,\el)).\\
\\
\fun{in}_2(\fun{ext}(),s) & \rightarrow & s;\\
\fun{in}_2(\fun{bst}(x,t_1,t_2),s) & \rightarrow
  & \fun{in}_2(t_1,\cons{x}{\fun{in}_2(t_2,s)}).\\
\\
\fun{ord}(\cons{x,y}{s}) & \rightarrow & \fun{ord}(\cons{y}{s}),
\;\text{si \(y \succ x\)};\\
\fun{ord}(\cons{x,y}{s}) & \rightarrow & \fun{false}();\\
\fun{ord}(s) & \rightarrow & \fun{true}().
\end{array}}
\end{equation*}
\caption{Vérification na\"{\i}ve d'un arbre binaire de recherche
\label{fig:bst0}}
\end{figure}
où \fun{in\(_2\)/2}\index{in2@\fun{in\(_2\)/2}} est simplement une
redéfinition de \fun{in/2}\index{in@\fun{in/2}} à la
\fig~\vref{fig:in}. Donc, le coût de \(\fun{in}_2(t)\), quand
\(t\)~est de taille~\(n\), est \(\C{\fun{in}_2}{n} = \C{\fun{in}}{n} =
2n + 2\).

Le pire des cas pour \fun{ord/1}\index{ord@\fun{ord/1}} se produit
quand la pile est ordonnée de façon croissante, donc le coût maximum
est \(\W{\fun{ord}}{n} = n\), si \(n > 0\). Le meilleur des cas est
manifesté quand la première clé est supérieure à la deuxième, donc le
coût minimum est \(\B{\fun{ord}}{n} = 1\). En somme, nous avons
\(\B{\fun{bst}_0}{n} = 1 + (2n+2) + 1 = 2n + 4\) et
\(\W{\fun{bst}_0}{n} = 1 + (2n+2) + n = 3n + 3\).

Un meilleur dessein consiste à ne pas construire le parcours infixe, à
\emph{conserver seulement la plus petite clé jusqu'à présent}, en
supposant que l'arbre est traversé de droite à gauche, et à la
comparer avec la clé courante. Nous avons alors un problème au
commencement, parce que nous n'avons encore visité aucun n{\oe}ud. Une
méthode fréquemment employée pour gérer ces situations exceptionnelles
consiste à utiliser une \emph{sentinelle}\index{sentinelle}, qui est
une valeur factice. Ici, nous voudrions prendre \(+\infty\) comme
sentinelle, parce que toute clé serait alors inférieure, en
particulier la plus grande \emph{qui n'est pas connue}. (Si elle
l'était, nous la prendrions comme sentinelle.) Il est en fait facile
de représenter cette valeur infinie dans notre langage fonctionnel:
usons simplement d'un constructeur constant
\fun{infty/0}\index{infty@\fun{infty/0}} (anglais, \emph{infinity}) et
assurons-nous que nous traitons ses comparaisons séparément des
autres. En réalité, \(\fun{infty}()\)\index{infty@\fun{infty/0}} n'est
comparé qu'une fois, avec la clé la plus grande, mais nous
n'essaierons pas d'optimiser cela, par peur d'obscurcir le dessein.

Le programme est montré à la
\fig~\vref{fig:bst}.\index{bst@\fun{bst/1}}\index{bst1@\fun{bst\(_1\)/2}}
\begin{figure}[t]
\begin{equation*}
\boxed{
\begin{array}{r@{\;}l@{\;}l}
  \fun{bst}(t) & \rightarrow & \fun{norm}(\fun{bst}_1(t,\fun{infty}())).\\
  \\
  \fun{bst}_1(\fun{ext}(),m) & \rightarrow & m;\\
  \fun{bst}_1(\fun{bst}(x,t_1,t_2),m) & \rightarrow &
  \fun{cmp}(x,t_1,\fun{bst}_1(t_2,m)).\\
\\
\fun{cmp}(x,t_1,\fun{infty}()) & \rightarrow & \fun{bst}_1(t_1,x);\\
\fun{cmp}(x,t_1,m) & \rightarrow &
  \fun{bst}_1(t_1,x), \,\text{si \(m \succ x\)};\\
\fun{cmp}(x,t_1,m) & \rightarrow & \fun{false}().\\
\\
\fun{norm}(\fun{false}()) & \rightarrow & \fun{false}();\\
\fun{norm}(m) & \rightarrow & \fun{true}().
\end{array}}
\end{equation*}
\caption{Vérification d'un arbre binaire de recherche\label{fig:bst}}
\end{figure}
Le paramètre~\(m\) représente la clé \emph{minimum} jusqu'à
présent. Le seul but de \fun{norm/1}\index{norm@\fun{norm/1}} est de
se débarrasser de la plus petite clé~\(m\) dans l'arbre et de terminer
à la place avec \(\fun{true}()\), mais, si l'arbre n'est pas vide,
nous pourrions tout aussi bien finir avec \(\fun{true}(m)\), ou même
\(\fun{false}(x)\), si plus d'information se révélait
utile.\index{true@\fun{true/1}}

Dans le pire des cas, l'arbre originel est un arbre binaire de
recherche, donc il doit être traversé dans son intégralité. S'il y a
\(n\)~n{\oe}uds internes, le coût maximum est \(\W{\fun{bst}}{n} = 1 +
2n + (n+1) + 1 = 3n + 3\)\index{bst@$\W{\fun{bst}}{n}$} parce que
chaque n{\oe}ud interne déclenche un appel à
\fun{bst\(_1\)/2}\index{bst1@\fun{bst\(_1\)/2}} et, à son tour, un
appel à \fun{cmp/3}\index{cmp@\fun{cmp/3}}; de même, tous les \(n+1\)
n{\oe}uds externes sont visités. Par conséquent, \(\W{\fun{bst}_0}{n}
= \W{\fun{bst}}{n}\),\index{bst0@$\W{\fun{bst}_0}{n}$} si \(n > 0\),
ce qui n'est pas une amélioration. Néanmoins, ici, nous ne
construisons pas une pile avec toutes les clés, ce qui est un gain
clair en termes de mémoire allouée.

Ceci dit, la mémoire n'est pas le seul avantage, car le coût minimum
de \fun{bst/1}\index{bst@\fun{bst/1}} est inférieur à celui de
\fun{bst\(_0\)/1}.\index{bst0@\fun{bst\(_0\)/1}} En effet, le meilleur
des cas pour les deux se produit lorsque l'arbre n'est pas un arbre
binaire de recherche, mais ceci est découvert au plus tôt par
\fun{bst/1} dès la deuxième comparaison, parce que la première est
toujours positive par dessein (\(+\infty \succ x\)). Bien entendu,
pour que la deuxième comparaison soit effectuée le plus tôt possible,
il faut que la première se produise le plus tôt possible aussi. Deux
configurations font l'affaire:
\begin{align*}
\fun{bst}(\fun{bst}(x,t_1,\fun{bst}(y,\fun{ext}(),\fun{ext}())))
& \xrightarrow{\smash{8}} \fun{false}(),\\
\fun{bst}(\fun{bst}(y,\fun{bst}(x,t_1,\fun{ext}()),\fun{ext}()))
& \xrightarrow{\smash{8}} \fun{false}(),
\end{align*}
où \(x \succcurlyeq y\). (Le deuxième arbre est la rotation à gauche
du premier. Nous avons vu \vpageref{par:rotation} que les parcours
infixes sont invariants par rotations.) Le coût minimum dans les deux
cas est \(\B{\fun{bst}}{n} = 8\),\index{bst@$\B{\fun{bst}}{n}$} à
contraster avec le coût linéaire \(\B{\fun{bst}_0}{n} = 2n +
4\)\index{bst0@$\B{\fun{bst}_0}{n}$} dû à l'inévitable parcours infixe
complet.

\section{Recherche}

Nous devons maintenant déterminer si la recherche d'une clé est plus
rapide qu'avec un arbre binaire ordinaire, ce qui était notre
motivation initiale. Étant donné l'arbre de recherche
\(\fun{bst}(x,t_1,t_2)\), si la clé~\(y\) que nous cherchons est telle
que \(y \succ x\), alors nous la recherchons récursivement
dans~\(t_2\); sinon, si \(x \succ y\), nous examinons~\(t_1\);
finalement, si \(y = x\), c'est que nous venons de la trouver à la
racine de l'arbre donné. La définition de \fun{mem/2} (anglais,
\emph{membership}) est donnée à la \fig~\vref{fig:mem}.
\begin{figure}
\begin{equation*}
\boxed{
\begin{array}{r@{\;}l@{\;}l}
\fun{mem}(y,\fun{ext}()) & \rightarrow & \fun{false}();\\
\fun{mem}(x,\fun{bst}(x,t_1,t_2)) & \rightarrow & \fun{true}();\\
\fun{mem}(y,\fun{bst}(x,t_1,t_2)) & \rightarrow &
  \fun{mem}(y,t_1), \; \text{si \(x \succ y\)};\\
\fun{mem}(y,\fun{bst}(x,t_1,t_2)) & \rightarrow &
  \fun{mem}(y,t_2).
\end{array}}
\end{equation*}
\caption{Appartenance à un arbre binaire de recherche\label{fig:mem}}
\end{figure}
Le point crucial est que nous ne sommes pas forcément amenés à visiter
tous les n{\oe}uds. Plus précisément, si tous les n{\oe}uds sont
visités, alors l'arbre est dégénéré\index{arbre binaire!$\sim$
  dégénéré}, à savoir, il est isomorphe à une pile, comme les arbres à
la \fig~\vref{fig:tree_stack} et~\vref{fig:zigzag}. Il est évident que
le coût minimum d'une recherche positive (la clé est trouvée) se
produit quand la clé est à la racine, donc
\(\B{\fun{mem}}{n{\scriptscriptstyle (+)}} =
1\),\index{mem@$\B{\fun{mem}}{n{\scriptscriptstyle (+)}}$} et une
recherche négative minimise son coût si la racine possède un enfant
qui est un n{\oe}ud externe visité:
\(\B{\fun{mem}}{n{\scriptscriptstyle (-)}} =
2\).\index{mem@$\B{\fun{mem}}{n{\scriptscriptstyle (-)}}$} Une
recherche positive maximise son coût quand l'arbre est dégénéré et la
clé en question est la seule feuille\index{arbre binaire!feuille}, donc
\(\W{\fun{mem}}{n{\scriptscriptstyle (+)}} =
n\),\index{mem@$\W{\fun{mem}}{n{\scriptscriptstyle (+)}}$} et une
recherche négative de coût maximum se produit si nous visitons un des
enfants de la feuille d'un arbre dégénéré\index{arbre binaire!$\sim$
  dégénéré}: \(\W{\fun{mem}}{n{\scriptscriptstyle (-)}} =
n+1\).\index{mem@$\W{\fun{mem}}{n{\scriptscriptstyle (-)}}$}
Par conséquent,
\begin{equation*}
\B{\fun{mem}}{n} = 1\quad\text{et}\quad \W{\fun{mem}}{n} = n + 1.
\index{mem@$\B{\fun{mem}}{n}$}\index{mem@$\W{\fun{mem}}{n}$}
\end{equation*}
Ces extremums sont les mêmes qu'avec une recherche
linéaire\index{recherche linéaire} par
\fun{ls/2}\index{ls@\fun{ls/2}}:
\begin{equation*}
\fun{ls}(x,\el)          \rightarrow  \fun{false}();\quad
\fun{ls}(x,\cons{x}{s})  \rightarrow  \fun{true}();\quad
\fun{ls}(x,\cons{y}{s})  \rightarrow  \fun{ls}(x,s).
\end{equation*}
Le coût d'une recherche linéaire positive est \(\C{\fun{ls}}{n,k} =
k\), si la clé recherchée est à la position~\(k\), avec la première
clé à la position~\(1\). Donc, en supposant que chaque clé est
distincte et également probable, le coût moyen d'une recherche
linéaire positive est \(\M{\fun{ls}}{n} =
\frac{1}{n}\sum_{k=1}^{n}\C{\fun{ls}}{n,k} =
(n+1)/2\).\index{recherche linéaire}\index{ls@$\M{\fun{ls}}{n}$} Ceci
soulève la question du coût moyen de
\fun{mem/2}.\index{mem@\fun{mem/2}}

\mypar{Coût moyen}

Il est clair d'après la définition qu'une recherche commence à la
racine et s'achève à un n{\oe}ud interne, en cas de succès, ou bien à
un n{\oe}ud externe si elle est infructueuse; de plus, chaque n{\oe}ud
sur ces chemins correspond à un appel de fonction. Par conséquent, le
coût moyen de \fun{mem/2}\index{mem@\fun{mem/2}} est en relation
directe avec la longueur interne moyenne\index{arbre binaire!longueur
  interne moyenne} et la longueur externe moyenne.\index{arbre
  binaire!longueur externe moyenne} Pour comprendre clairement
comment, considérons un arbre binaire de recherche de taille~\(n\)
contenant des clés distinctes. Le coût total de la recherche de toutes
ces clés est \(n+I_n\), où \(I_n\)~est la longueur interne (nous
ajoutons~\(n\) à~\(I_n\) parce que nous comptons les n{\oe}uds sur les
chemins, pas les arcs, car un n{\oe}ud interne est associé à un appel
de fonction.)  En d'autres termes, une clé choisie au hasard parmi
celles que nous savons présentes dans un arbre donné de taille~\(n\)
est trouvée par \fun{mem/2} pour un coût moyen de \(1+I_n/n\). Par
dualité, le coût total pour atteindre tous les n{\oe}uds externes d'un
arbre binaire de recherche est \((n+1)+E_n\), où \(E_n\)~est la
longueur externe (il y a \(n+1\) n{\oe}uds externes dans un arbre
contenant \(n\)~n{\oe}uds internes; voir le théorème~\ref{thm_int_ext}
\vpageref{thm_int_ext}). Autrement dit, le coût moyen d'une recherche
négative (la clé est absente) avec \fun{mem/2}\index{mem@\fun{mem/2}}
est \(1 + E_n/(n+1)\).

Parvenu à ce point, nous devrions comprendre que nous avons affaire à
deux processus aléatoires, ou, de manière équivalente, à une moyenne
de moyennes. En effet, la discussion précédente supposait que l'arbre
de recherche était donné, mais que la clé était aléatoire. Le cas
général est quand les deux sont aléatoires, c'est-à-dire que les
résultats précédents sont moyennés pour tous les arbres possibles de
taille fixe~\(n\). Soit \(\M{\fun{mem}}{n{\scriptscriptstyle (+)}}\)
\index{mem@$\M{\fun{mem}}{n{\scriptscriptstyle (+)}}$} le coût moyen
de la recherche positive d'une clé aléatoire dans un arbre aléatoire de
taille~\(n\) (chacune des \(n\)~clés étant recherchée avec la même
probabilité); de plus, soit \(\M{\fun{mem}}{n{\scriptscriptstyle
    (-)}}\)\index{mem@$\M{\fun{mem}}{n{\scriptscriptstyle (-)}}$} le
coût moyen de la recherche négative d'une clé aléatoire dans un arbre
aléatoire (chacun des \(n+1\) intervalles dont les extrémités sont les
\(n\)~clés ont la même probabilité d'être atteints). Alors
\begin{equation}
  \M{\fun{mem}}{n{\scriptscriptstyle (+)}}
  = 1 + \frac{1}{n}\Expected{I_n}
\quad\text{et}\quad
\M{\fun{mem}}{n{\scriptscriptstyle (-)}}
  = 1 + \frac{1}{n+1}\Expected{E_n},
\label{eq_Mmems}
\end{equation}
où \(\Expected{I_n}\) et \(\Expected{E_n}\) sont, respectivement, la
longueur interne moyenne\index{arbre binaire!longueur interne moyenne}
et la longueur externe moyenne.\index{arbre binaire!longueur externe
  moyenne} En réutilisant l'équation~\eqref{eq_EI},
page~\pageref{eq_EI} (\(E_n = I_n + 2n\)), nous déduisons
\(\Expected{E_n} = \Expected{I_n} + 2n\) et nous pouvons maintenant
mettre en relation les coûts moyens de la recherche en éliminant les
longueurs moyennes:
\begin{equation}
\M{\fun{mem}}{n{\scriptscriptstyle (+)}} = \left(1 + \frac{1}{n}\right)
\M{\fun{mem}}{n{\scriptscriptstyle (-)}} - \frac{1}{n} - 2.
\label{eq_Mmem}
\end{equation}
Il est remarquable que cette équation est valable pour tous les arbres
binaires de recherche, \emph{indépendamment de la manière dont ils ont
  été construits}. Dans la section suivante, nous envisagerons deux
méthodes pour faire des arbres de recherche et nous serons à même de
déterminer \(\M{\fun{mem}}{n{\scriptscriptstyle (+)}}\)
\index{mem@$\M{\fun{mem}}{n{\scriptscriptstyle (+)}}$} et
\(\M{\fun{mem}}{n{\scriptscriptstyle (-)}}\)
\index{mem@$\M{\fun{mem}}{n{\scriptscriptstyle (-)}}$} avec l'aide de
l'équation~\eqref{eq_Mmem}.

Mais avant, nous pourrions peut-être remarquer qu'à la
\fig~\vref{fig:mem} nous n'avons pas suivi l'ordre des comparaisons
tel que nous l'avions décrit. Dans le cas d'une recherche positive, la
comparaison \(y = x\) est vraie exactement une fois, à la fin; par
conséquent, la vérifier avant les autres, comme nous l'avons fait dans
la deuxième règle à la \fig~\vref{fig:mem}, signifie qu'elle échoue
pour toute clé sur le chemin de recherche, sauf la dernière. Si nous
mesurons le coût en tant que nombre d'appels de fonction, cela ne
change rien, mais, si nous sommes intéressés par la minimisation du
nombre de comparaisons d'une recherche, il est préférable de déplacer
cette règle \emph{après} les autres tests d'inégalité, comme à la
\fig~\vref{fig:mem0}.
\begin{figure}
\begin{equation*}
\boxed{
\begin{array}{r@{\;}l@{\;}l}
\fun{mem}_0(y,\fun{bst}(x,t_1,t_2)) & \rightarrow &
  \fun{mem}_0(y,t_1), \; \text{si \(x \succ y\)};\\
\fun{mem}_0(y,\fun{bst}(x,t_1,t_2)) & \rightarrow &
  \fun{mem}_0(y,t_2),\; \text{si \(y \succ x\)};\\
\fun{mem}_0(y,\fun{ext}()) & \rightarrow & \fun{false}();\\
\fun{mem}_0(y,t) & \rightarrow & \fun{true}().
\end{array}}
\end{equation*}
\caption{Chercher avec moins de comparaisons bivaluées\label{fig:mem0}}
\end{figure}
(Nous supposons qu'une égalité est vérifiée aussi vite qu'une
inégalité.) Avec \fun{mem\(_0\)/2}\index{mem0@\fun{mem\(_0\)/2}}, le
nombre de comparaisons pour chaque chemin de recherche est différent à
cause de l'asymétrie entre la gauche et la droite: la visite
de~\(t_1\) cause une comparaison (\(x \succ y\)), alors que
\(t_2\)~déclenche deux comparaisons (\(x \nsucc y\) et \(y \succ
x\)). De plus, nous avons aussi déplacé le motif pour le n{\oe}ud
externe après les règles avec des comparaisons, parce que chaque
chemin de recherche contient exactement un n{\oe}ud externe à la fin,
donc il est probablement plus efficace de vérifier cela en dernier. En
passant, tous les manuels dont nous avons connaissance supposent
qu'une seule comparaison atomique avec trois résultats possibles (une
\emph{comparaison trivaluée} résulte en «~inférieur~», «~supérieur~» ou
«~égal~») a lieu, bien que les programmes qu'ils donnent emploient
explicitement les \emph{comparaisons bivaluées} \((=)\) et
\((\succ)\). Ce point aveugle général rend toute analyse théorique
fondée sur le nombre de comparaisons moins pertinente, parce que la
plupart des langages de programmation de haut niveau n'offrent tout
simplement pas de comparaison trivaluée de façon native.

\mypar{La variante d'Andersson}

\cite{Andersson_1991} a proposé une variante de la recherche qui prend
explicitement en compte l'usage de comparaisons bivaluées et réduit
leur nombre au minimum, aux dépens de plus d'appels de fonctions. Le
dessein consiste à passer d'un appel à l'autre une clé candidate, tout
en descendant dans l'arbre, et à toujours terminer une recherche à un
n{\oe}ud externe: si la candidate égale alors la clé recherchée, la
recherche est positive, sinon elle ne l'est pas. Par conséquent, le
coût en termes d'appels de fonctions d'une recherche négative est le
même qu'avec \fun{mem/2}\index{mem@\fun{mem/2}} ou
\fun{mem\(_0\)/2}\index{mem0@\fun{mem\(_0\)/2}}, et le n{\oe}ud externe
terminal est le même, mais le coût d'une recherche positive est plus
élevé. Néanmoins, l'avantage est que \emph{l'égalité n'est pas testée
  à la descente}, seulement quand le n{\oe}ud externe est atteint,
donc seulement une comparaison par n{\oe}ud est nécessaire. Le
programme est montré à la \fig~\vref{fig:mem1}.
\begin{figure}
\begin{equation*}
\boxed{
\begin{array}{r@{\;}l@{\;}l}
\fun{mem}_1(y,\fun{bst}(x,t_1,t_2)) & \rightarrow &
  \fun{mem}_2(y,\fun{bst}(x,t_1,t_2),x);\\
\fun{mem}_1(y,\fun{ext}()) & \rightarrow & \fun{false}().\\
\\
\fun{mem}_2(y,\fun{bst}(x,t_1,t_2),c) & \rightarrow &
  \fun{mem}_2(y,t_1,c),\; \text{si \(x \succ y\)};\\
\fun{mem}_2(y,\fun{bst}(x,t_1,t_2),c) & \rightarrow &
  \fun{mem}_2(y,t_2,x);\\
\fun{mem}_2(y,\fun{ext}(),y) & \rightarrow & \fun{true}();\\
\fun{mem}_2(y,\fun{ext}(),c) & \rightarrow & \fun{false}().
\end{array}}
\end{equation*}
\caption{Recherche à la Andersson (clé candidate)\label{fig:mem1}}
\end{figure}
La candidate est le troisième argument de
\fun{mem\(_2\)/2}\index{mem2@\fun{mem\(_2\)/2}} et sa première valeur
est la racine de l'arbre lui-même, comme nous pouvons le voir à la
première règle de \fun{mem\(_1\)/2}\index{mem1@\fun{mem\(_1\)/2}}. La
seule différence conceptuelle avec
\fun{mem\(_0\)/2}\index{mem0@\fun{mem\(_0\)/2}} est la manière dont
une recherche positive est identifiée: si, quelque part lors de la
descente, \(x = y \), alors \(x\)~devient la candidate et elle sera
passée aux autres appels («~vers le bas~») jusqu'à un n{\oe}ud externe
où \(x = x\).

Le pire des cas se produit quand l'arbre est dégénéré\index{arbre
  binaire!$\sim$ dégénéré} et \fun{mem\(_1\)/2} effectue \(n+1\)
comparaisons bivaluées, soit \(\OW{\fun{mem}_1}{n} = n +
1\),\index{mem1@$\OW{\fun{mem}_1}{n}$} d'après les notations que nous
avons utilisées dans l'analyse du tri par interclassement\index{tri
  par interclassement}, au chapitre~\ref{chap:merge_sort}
\vpageref{chap:merge_sort}.

Dans le cas de \fun{mem\(_0\)/2}, l'appel récursif au sous-arbre droit
entraîne deux fois plus de comparaisons qu'au sous-arbre gauche, donc
le pire des cas est un arbre dégénéré penchant à droite, comme celui à
la \fig~\vref{fig:min_pre0}, tous les n{\oe}uds internes étant
visités: \(\OW{\fun{mem}_0}{n} = 2n\).\index{mem0@$\OW{\fun{mem}_0}{n}$}

Dans le cas de \fun{mem/2}, le nombre de comparaisons est symétrique
parce que l'égalité est vérifiée d'abord, donc le pire des cas est un
arbre dégénéré dans lequel une recherche négative amène à visiter tous
les n{\oe}uds internes et un n{\oe}ud externe: \(\OW{\fun{mem}}{n} =
2n + 1\).\index{mem@$\OW{\fun{mem}}{n}$} Asymptotiquement, nous avons
\begin{equation*}
\OW{\fun{mem}}{n} \sim \OW{\fun{mem}_0}{n}
\sim 2 \cdot \OW{\fun{mem}_1}{n}.
\end{equation*}

Dans le cas de la recherche à la Andersson, il n'y a pas de différence
de coût, en termes d'appels de fonction, entre une recherche positive
et négative, donc, si \(n > 0\), nous avons
\begin{equation}
\M{\fun{mem}_3}{n} = 1 + \M{\fun{mem}_2}{n}\quad\text{et}\quad
\M{\fun{mem}_2}{n} = \M{\fun{mem}}{n{\scriptscriptstyle (-)}}.
\label{eq_Andersson_average}
\index{mem3@$\M{\fun{mem}_3}{n}$}\index{mem2@$\M{\fun{mem}_2}{n}$}
\end{equation}
Le choix entre \fun{mem\(_0\)/2} et \fun{mem\(_1\)/2} dépend du
compilateur ou de l'interprète du langage de programmation choisi pour
la réalisation. Si une comparaison bivaluée est plus lente qu'une
indirection (suivre un pointeur, ou, au niveau du langage
d'assemblage, effectuer un saut inconditionnel), il est probablement
judicieux d'opter pour la variante d'Andersson. Mais le jugement final
doit aussi s'appuyer sur un jeu de tests.

Finalement, nous pouvons simplifier le programme d'Andersson en nous
débarrassant du test de la valeur \(\fun{ext}()\) au début de
\fun{mem\(_1\)/2}.\index{mem1@\fun{mem\(_1\)/2}} Nous devons alors
simplement avoir un sous-arbre candidat, dont la racine est la clé
candidate dans le programme originel. Voir \fig~\vref{fig:mem3}
\begin{figure}
\begin{equation*}
\boxed{
\begin{array}{r@{\;}l@{\;}l}
  \fun{mem}_3(y,t) & \rightarrow & \fun{mem}_4(y,t,t).\\
  \\
  \fun{mem}_4(y,\fun{bst}(x,t_1,t_2),t) & \rightarrow &
  \fun{mem}_4(y,t_1,t),\; \text{si \(x \succ y\)};\\
\fun{mem}_4(y,\fun{bst}(x,t_1,t_2),t) & \rightarrow &
  \fun{mem}_4(y,t_2,\fun{bst}(x,t_1,t_2));\\
\fun{mem}_4(y,\fun{ext}(),\fun{bst}(y,t_1,t_2)) & \rightarrow & \fun{true}();\\
\fun{mem}_4(y,\fun{ext}(),t) & \rightarrow & \fun{false}().
\end{array}}
\end{equation*}
\caption{Recherche à la Andersson (arbre candidat)\label{fig:mem3}}
\end{figure}
où nous avons \(\OW{\fun{mem}_3}{n} = \OW{\fun{mem}_1}{n} = n + 1\).
Cette version ne devrait être privilégiée que si le langage de
programmation employé pour la mise en {\oe}uvre possède des
\emph{synonymes}\index{mémoire!synonymie}, définis dans les motifs, ou,
de façon équivalente, si le compilateur peut détecter que le terme
\(\fun{bst}(x,t_1,t_2)\) peut être partagé au lieu d'être dupliqué
dans la deuxième règle de \fun{mem\(_4\)/3} (ici, nous supposons que
la partage est implicite et maximal pour chaque
règle).\index{mem4@\fun{mem\(_4\)/3}} Pour des informations
supplémentaires sur la variante d'Andersson, nous recommandons la
lecture de \cite{Spuler_1992}.


\section{Insertion}
\label{sec:bst_insertion}

\mypar{Insertion de feuilles}\index{arbre binaire de
  recherche!insertion de feuilles|(}

Puisque toutes les recherches se terminent à un n{\oe}ud externe, il
est extrêmement tentant de commencer l'insertion d'une clé distincte
par une recherche négative et ensuite remplacer le n{\oe}ud externe
par une feuille contenant la clé à ajouter. La \fig~\vref{fig:insl}
\begin{figure}[b]
\begin{equation*}
\boxed{
\begin{array}{r@{\;}l@{\;}l}
\fun{insl}(y,\fun{bst}(x,t_1,t_2)) & \xrightarrow{\smash{\tau}} &
  \fun{bst}(x,\fun{insl}(y,t_1),t_2), \; \text{si \(x \succ y\)};\\
\fun{insl}(y,\fun{bst}(x,t_1,t_2)) & \xrightarrow{\smash{\upsilon}} &
  \fun{bst}(x,t_1,\fun{insl}(y,t_2));\\
\fun{insl}(y,\fun{ext}()) & \xrightarrow{\smash{\phi}} & \fun{bst}(y,\fun{ext}(),\fun{ext}()).
\end{array}}
\end{equation*}
\caption{Insertion de feuilles avec doublons possibles\label{fig:insl}}
\end{figure}
montre le programme pour \fun{insl/2}\index{insl@\fun{insl/2}}
(anglais, \emph{insert a leaf}). Remarquons qu'il permet la présence
de clés en doublon, ce qui complique considérablement l'analyse du
coût \citep{Burge_1976,ArchibaldClement_2006,Pasanen_2010}.  La
\fig~\vref{fig:insl0}
\begin{figure}
\begin{equation*}
\boxed{
\begin{array}{r@{\;}l@{\;}l}
\fun{insl}_0(y,\fun{bst}(x,t_1,t_2)) & \rightarrow &
  \fun{bst}(x,\fun{insl}_0(y,t_1),t_2), \; \text{si \(x \succ y\)};\\
\fun{insl}_0(y,\fun{bst}(x,t_1,t_2)) & \rightarrow &
  \fun{bst}(x,t_1,\fun{insl}_0(y,t_2)) , \; \text{si \(y \succ x\)};\\
\fun{insl}_0(y,\fun{ext}()) & \rightarrow &
\fun{bst}(y,\fun{ext}(),\fun{ext}());\\
\fun{insl}_0(y,t) & \rightarrow & t.
\end{array}}
\end{equation*}
\caption{Insertion de feuilles sans doublons\label{fig:insl0}}
\end{figure}
montre une variante qui maintient l'unicité des clés, basée sur la
définition de \fun{mem\(_0\)/2}\index{mem0@\fun{mem\(_0\)/2}} à la
\fig~\vref{fig:mem0}. Nous pouvons aussi réutiliser la recherche à la
Andersson pour inspirer une autre solution à la
\fig~\vref{fig:insl1}.\index{insl1@\fun{insl/1}}
%\bigskip
%\bigskip
%\bigskip
\begin{figure}
\begin{equation*}
\boxed{
\begin{array}{r@{\;}l@{\;}l}
\fun{insl}_1(y,t) & \rightarrow & \fun{insl}_2(y,t,t).\\
\\
\fun{insl}_2(y,\fun{bst}(x,t_1,t_2),t) & \rightarrow &
  \fun{bst}(x,\fun{insl}_2(y,t_1,t),t_2), \; \text{si \(x \succ y\)};\\
\fun{insl}_2(y,\fun{bst}(x,t_1,t_2),t) & \rightarrow &
  \fun{bst}(x,t_1,\fun{insl}_2(y,t_2,\fun{bst}(x,t_1,t_2)));\\
\fun{insl}_2(y,\fun{ext}(),\fun{bst}(y,t_1,t_2)) & \rightarrow & \fun{ext}();\\
\fun{insl}_2(y,\fun{ext}(),t) & \rightarrow & \fun{bst}(y,\fun{ext}(),\fun{ext}()).
\end{array}}
\end{equation*}
\caption{Insertion à la Andersson\label{fig:insl1}}
\end{figure}

\mypar{Coût moyen}

Dans le but de mener à bien l'analyse du coût moyen de l'insertion de
feuilles, nous devons supposer que toutes les clés sont distinctes
deux à deux; cela équivaut à considérer tous les arbres de recherche
résultant de l'insertion dans un arbre vide au début de toutes les
clés de chaque permutation de \((1,2,\dots,n)\). Étant donné que le
nombre de permutations est supérieur au nombre d'arbres de même
taille, car \(n! > b_n\) si \(n > 2\) (équation~\eqref{eq_Cn}
\vpageref{eq_Cn}), nous nous attendons à ce que certaines formes
d'arbre correspondent à plusieurs permutations. Comme nous le verrons
à la section à propos de la taille moyenne, les arbres dégénérés et
extrêmement déséquilibrés sont rares en moyenne \citep{Fill_1996}, ce
qui fait des arbres binaires de recherche une bonne structure de
donnée si les données sont aléatoires, en tout cas tant que des
insertions de feuilles sont pratiquées. Nous n'allons considérer que
\fun{insl/2} dans ce qui suit, parce que nous supposerons que les clés
insérées sont uniques. (L'insertion à la Andersson n'est intéressante
que si des clés peuvent être répétées en entrée et doivent être
identifiées, laissant l'arbre de recherche invariant.)

\hspace*{-3pt} Définissons une fonction \fun{mkl/1}\index{mkl@\fun{mkl/1}}
\index{mkl@\fun{mkl/2}} (anglais, \emph{make leaves}) à la
\fig~\vref{fig:mkl}
\begin{figure}
\begin{equation*}
\boxed{
\begin{array}{@{}r@{\;}l@{\;}lr@{\;}l@{\;}l@{}}
  \fun{mkl}(s) & \xrightarrow{\smash{\xi}} & \fun{mkl}(s,\fun{ext}()).
& \fun{mkl}(\el,t) & \xrightarrow{\smash{\psi}} & t;\\
& & & \fun{mkl}(\cons{x}{s},t) & \xrightarrow{\smash{\omega}} & \fun{mkl}(s,\fun{insl}(x,t)).
\end{array}}
\end{equation*}
\caption{Créer un arbre avec des insertions de feuilles\label{fig:mkl}}
\end{figure}
qui construit un arbre binaire de recherche en insérant des feuilles
dont le contenu est donné dans une pile. Remarquons que nous pourrions
aussi définir une fonction \fun{mklR/1}\index{mklR@\fun{mklR/1}}
(anglais, \emph{make leaves in reverse order}) telle que
\(\fun{mklR}(s) \equiv \fun{mkl}(\fun{rev}(s))\), d'une manière
compacte:
\begin{equation}
%\abovedisplayskip=4pt
%\belowdisplayskip=4pt
  \fun{mklR}(\el) \rightarrow \fun{ext}();
  \quad
  \fun{mklR}(\cons{x}{s}) \rightarrow \fun{insl}(x,\fun{mklR}(s)).
\label{eq_mklR}
\end{equation}
Le coût de \(\fun{insl}(x,t)\) dépend de~\(x\) et de la forme
de~\(t\), mais le coût moyen
\(\M{\fun{insl}}{k}\)\index{insl@$\M{\fun{insl}}{k}$} de
\(\fun{insl}(x,t)\) dépend seulement de la taille~\(k\) des arbres,
parce que toutes les formes sont créées par \(\fun{mkl}(s)
\twoheadrightarrow t\) pour une longueur donnée de~\(s\), et tous les
n{\oe}uds externes de~\(t\) ont une égale probabilité d'être remplacés
par une feuille contenant~\(x\). Précisément:
\begin{equation}
\abovedisplayskip=2pt
\abovedisplayshortskip=2pt
\belowdisplayskip=4pt
\M{\fun{mkl}}{n} = 2 + \sum_{k=0}^{n-1}\M{\fun{insl}}{k}.
\label{eq_Mmkl0}
\end{equation}
Une caractéristique remarquable de l'insertion de feuilles est que les
n{\oe}uds internes ne bougent pas, donc la longueur
interne\index{arbre binaire de recherche!longueur interne} des
n{\oe}uds est invariante et le coût de rechercher toutes les clés d'un
arbre de taille~\(n\) est le coût de les avoir insérées
auparavant. Nous avons déjà remarqué que le coût moyen de la recherche
est \(n + \Expected{I_n}\); le coût moyen des \(n\)~insertions est
\(\sum_{k=0}^{n-1}\M{\fun{insl}}{k}\). Donc d'après
l'équation~\eqref{eq_Mmkl0}:
\begin{equation}
%\abovedisplayskip=2pt
%\belowdisplayskip=4pt
n + \Expected{I_n} = \M{\fun{mkl}}{n} - 2
\label{eq_n_EIn}
\end{equation}
(La soustraction de~\(2\) tient compte des règles~\(\smash{\xi}\)
et~\(\smash{\psi}\), qui n'effectuent aucune insertion.) Le coût de
l'insertion d'une feuille est celui d'une recherche négative:
\begin{equation}
%\abovedisplayskip=4pt
%\belowdisplayskip=4pt
\M{\fun{insl}}{k} = \M{\fun{mem}}{k{\scriptscriptstyle (-)}}.
\label{eq_n_plus_EIn}
\end{equation}
Nous souvenant de l'équation~\eqref{eq_Mmems} \vpageref{eq_Mmems},
et aussi des équations~\eqref{eq_Mmkl0}, \eqref{eq_n_EIn}
et~\eqref{eq_n_plus_EIn}, il vient:
\begin{equation*}
\abovedisplayskip=2pt
\belowdisplayskip=4pt
\M{\fun{mem}}{n{\scriptscriptstyle(+)}}
= 1 + \frac{1}{n}\Expected{I_n}
= \frac{1}{n}(\M{\fun{mkl}}{n} - 2)
= \frac{1}{n}\sum_{k=0}^{n-1}\M{\fun{insl}}{k}
= \frac{1}{n}\sum_{k=0}^{n-1}\M{\fun{mem}}{k{\scriptscriptstyle (-)}}.
\end{equation*}
Finalement, en utilisant l'équation~\eqref{eq_Mmem}
\vpageref{eq_Mmem}, nous déduisons
\begin{equation*}
\abovedisplayskip=0pt
\belowdisplayskip=0pt
\frac{1}{n}\sum_{k=0}^{n-1}\M{\fun{mem}}{k{\scriptscriptstyle (-)}}
=
\left(1 + \frac{1}{n}\right)
\M{\fun{mem}}{n{\scriptscriptstyle (-)}} - \frac{1}{n} - 2.
\end{equation*}
De manière équivalente:
\begin{equation*}
\abovedisplayskip=-7pt
\abovedisplayshortskip=-7pt
2n + 1 + \sum_{k=0}^{n-1}\M{\fun{mem}}{k{\scriptscriptstyle (-)}}
= (n+1) \M{\fun{mem}}{n{\scriptscriptstyle (-)}}.
\end{equation*}
Cette équation est facile si nous soustrayons son instance \(n-1\):
\begin{equation*}
\abovedisplayskip=6pt
\belowdisplayskip=6pt
2 + \M{\fun{mem}}{n-1{\scriptscriptstyle(-)}} =
(n+1) \M{\fun{mem}}{n{\scriptscriptstyle (-)}}
- n \M{\fun{mem}}{n-1{\scriptscriptstyle (-)}}.
\end{equation*}
En remarquant que \(\M{\fun{mem}}{0{\scriptscriptstyle (-)}} = 1\),
l'équation devient
\begin{equation*}
\abovedisplayskip=0pt
\belowdisplayskip=0pt
\M{\fun{mem}}{0{\scriptscriptstyle (-)}} = 1,\quad
\M{\fun{mem}}{n{\scriptscriptstyle (-)}}
= \M{\fun{mem}}{n-1{\scriptscriptstyle (-)}} + \frac{2}{n+1},
\end{equation*}
donc
\begin{equation}
\abovedisplayskip=-6pt
\abovedisplayshortskip=-6pt
\M{\fun{mem}}{n{\scriptscriptstyle (-)}} =
1 + 2 \sum_{k=2}^{n+1}\frac{1}{k} = 2H_{n+1} - 1,
\index{mem@$\M{\fun{mem}}{n{\scriptscriptstyle (-)}}$}
\label{eq_Mmem_fail}
\end{equation}
où \(H_n := \sum_{k=1}^{n}1/k\)~est le \(n^\text{e}\) nombre
harmonique\index{nombre harmonique}. En remplaçant
\(\M{\fun{mem}}{n{\scriptscriptstyle (-)}}\) dans
l'équation~\eqref{eq_Mmem} et en utilisant \(H_{n+1} = H_n +
1/(n+1)\), on obtient
\begin{equation}
\M{\fun{mem}}{n{\scriptscriptstyle (+)}} =
2\left(1+\frac{1}{n}\right)H_n - 3.
\index{mem@$\M{\fun{mem}}{n{\scriptscriptstyle (+)}}$}
\label{eq_Mmem_ok}
\end{equation}
D'après les inéquations~\eqref{ineq_Hn} \vpageref{ineq_Hn} et
les équations~\eqref{eq_Mmem_fail} et~\eqref{eq_Mmem_ok}:
\begin{equation*}
\M{\fun{insl}}{n}
\sim \M{\fun{mem}}{n{\scriptscriptstyle (-)}}
\sim \M{\fun{mem}}{n{\scriptscriptstyle (+)}} \sim 2 \ln n.
\end{equation*}
Nous obtenons plus d'information sur les comportements asymptotiques
de \(\M{\fun{mem}}{n{\scriptscriptstyle(-)}}\) et
\(\M{\fun{mem}}{n{\scriptscriptstyle(+)}}\) en examinant leur
différence plutôt que leur rapport:
\begin{equation*}
\M{\fun{mem}}{n{\scriptscriptstyle (-)}} -
\M{\fun{mem}}{n{\scriptscriptstyle (+)}} =
\frac{2}{n}(n + 1 - H_{n+1}) \sim 2
\quad\text{et}\quad
1 \leqslant \M{\fun{mem}}{n{\scriptscriptstyle (-)}} -
\M{\fun{mem}}{n{\scriptscriptstyle (+)}} < 2.
\end{equation*}
La différence moyenne entre une recherche négative et une positive
tend lentement vers~\(2\) pour des valeurs croissantes de~\(n\), ce
qui peut ne pas être évident. Nous pouvons faire usage de ce résultat
pour comparer les différences moyennes des coûts d'une recherche
positive avec \fun{mem/2} et \fun{mem\(_3\)/2} (Andersson). Nous
souvenant de l'équation~\eqref{eq_Andersson_average}
\vpageref{eq_Andersson_average}, nous tirons \(1 +
\M{\fun{mem}}{n{\scriptscriptstyle (-)}} =
\M{\fun{mem}_3}{n{\scriptscriptstyle (+)}}\). Le résultat précédent
maintenant entraîne
\begin{equation*}
  \M{\fun{mem}_3}{n{\scriptscriptstyle (+)}} -
  \M{\fun{mem}}{n{\scriptscriptstyle (+)}} \sim 3.
\index{mem3@$\M{\fun{mem}_3}{n{\scriptscriptstyle (+)}}$}
\end{equation*}
Par conséquent, le coût additionnel de la variante d'Andersson dans le
cas d'une recherche positive est asymptotiquement~\(3\), en moyenne.

Par ailleurs, remplacer \(\M{\fun{mem}}{n{\scriptscriptstyle
    (-)}}\) et \(\M{\fun{mem}}{n{\scriptscriptstyle (+)}}\) dans
les équations~\eqref{eq_Mmems} donne
\begin{equation}
\Expected{I_n} = 2(n+1)H_n - 4n
\quad\text{et}\quad
\Expected{E_n} = 2(n+1)H_n - 2n.
\label{eq_IEn}
\end{equation}
Donc \(\Expected{I_n} \sim \Expected{E_n} \sim 2n\ln n\). Remarquons
combien il est facile de trouver~\(\Expected{I_n}\) pour des arbres
binaires de recherche, en comparaison avec l'extrême difficulté des
arbres binaires normaux.

Si nous nous intéressons à des résultats un petit plus théoriques,
nous pourrions rechercher le nombre moyen de comparaisons pour une
recherche et une insertion. Un coup d'{\oe}il à la \fig~\vref{fig:mem}
révèle que deux comparaisons bivaluées par n{\oe}ud interne sont
effectuées à la descente et une comparaison bivaluée (égalité) est
faite si la clé est atteinte, sinon aucune:
\begin{equation}
\OM{\fun{mem}}{n{\scriptscriptstyle(+)}}
  = 1 + \frac{2}{n}\Expected{I_n}
\quad\text{et}\quad
\OM{\fun{mem}}{n{\scriptscriptstyle(-)}}
  = \frac{2}{n+1}\Expected{E_n}.
\label{eq_Mmem_cmp1}
\end{equation}
En réutilisant les équations~\eqref{eq_IEn}, nous concluons que
\begin{equation}
\OM{\fun{mem}}{n{\scriptscriptstyle(+)}}
  = 4\left(1+\frac{1}{n}\right)H_n - 7
\quad\text{et}\quad
\OM{\fun{mem}}{n{\scriptscriptstyle(-)}}
  = 4H_n + \frac{4}{n+1} - 4.
\index{mem@$\OM{\fun{mem}}{n{\scriptscriptstyle(-)}}$}
\index{mem@$\OM{\fun{mem}}{n{\scriptscriptstyle(+)}}$}
\label{eq_Mmem_cmp2}
\end{equation}
Clairement, nous avons \(\OM{\fun{mem}}{n{\scriptscriptstyle(+)}} \sim
\OM{\fun{mem}}{n{\scriptscriptstyle(-)}} \sim 4\ln n\). De plus,
\begin{equation*}
\OM{\fun{mem}}{n{\scriptscriptstyle (-)}} -
\OM{\fun{mem}}{n{\scriptscriptstyle (+)}} =
\frac{4}{n+1} - \frac{4}{n}H_n + 3 \sim 3
\quad\text{et}\quad
1 \leqslant \OM{\fun{mem}}{n{\scriptscriptstyle (-)}} -
\OM{\fun{mem}}{n{\scriptscriptstyle (+)}} < 3.
\end{equation*}

Les coûts moyens pour la recherche à la Andersson et les insertions
sont faciles à déduire aussi, grâce aux
équations~\eqref{eq_Andersson_average} \vpageref{eq_Andersson_average}
et~\eqref{eq_Mmem_fail} à la page~\pageref{eq_Mmem_fail}:
\(\M{\fun{mem}_3}{n} = 2H_{n+1} \sim 2 \ln
n\).\index{mem3@$\M{\fun{mem}_3}{n}$} Un retour sur la
\fig~\vref{fig:mem3} met en exergue qu'une comparaison bivaluée (\(x
\succ y\)) est effectuée en descendant dans l'arbre et une de plus
pour l'arrêt à un n{\oe}ud externe, que la recherche soit positive ou
non:
\begin{equation*}
  \OM{\fun{mem}_3}{n} = \frac{1}{n+1}\Expected{E_n} =
  2H_n + \frac{2}{n+1} - 2 \sim 2\ln n.
\end{equation*}
Nous pouvons finalement comparer le nombre moyen de comparaisons entre
\fun{mem/2} et \fun{mem\(_3\)/2} (Andersson):
\begin{align*}
  \OM{\fun{mem}}{n{\scriptscriptstyle(+)}} - \OM{\fun{mem}_3}{n}
&= 2\left(1 + \frac{2}{n}\right)H_n - \frac{2}{n+1} - 5 \sim 2\ln n,\\
  \OM{\fun{mem}}{n{\scriptscriptstyle(-)}} - \OM{\fun{mem}_3}{n}
&= 2H_n + \frac{2}{n+1} - 2 \sim 2\ln n.
\end{align*}
En ce qui concerne l'insertion de feuilles,
\fun{insl/2}\index{insl@\fun{insl/2}} se comporte comme
\fun{mem\(_3\)/2}\index{mem3@\fun{mem\(_3\)/2}}, sauf qu'aucune
comparaison n'est faite aux n{\oe}uds externes. De plus, nous
finissons l'analyse du coût moyen de \(\fun{insl/2}\) avec les
équations~\eqref{eq_n_plus_EIn} et~\eqref{eq_Mmem_fail}:
\begin{equation*}
  \OM{\fun{insl}}{n} =
  \OM{\fun{mem}_3}{n} - 1 = 2H_n + \frac{2}{n+1} -
  3\quad\text{et}\quad \M{\fun{insl}}{n} = 2H_{n+1} - 1.
\index{insl@$\OM{\fun{insl}}{n}$}
\index{mem3@$\OM{\fun{mem}_3}{n}$}
\end{equation*}
Finalement, grâce aux équations~\eqref{eq_n_EIn} et~\eqref{eq_IEn},
nous déduisons
\begin{equation}
\M{\fun{mkl}}{n} = n + \Expected{I_n} + 2 = 2(n+1)H_n - n + 2
\sim 2n\ln n.
\index{mkl@$\M{\fun{mkl}}{n}$}
\label{eq_Mmkl}
\end{equation}

\mypar{Coût amorti}

Le pire des cas pour l'insertion d'une feuille se produit quand
l'arbre de recherche est dégénéré\index{arbre binaire!$\sim$ dégénéré}
et la clé à insérer devient la feuille la plus éloignée de la racine.
Si l'arbre a pour taille~\(n\), alors \(n+1\) appels sont effectués,
comme nous pouvons le voir à la \fig~\vref{fig:insl}, donc
\(\W{\fun{insl}}{n} = n + 1\)\index{insl@$\W{\fun{insl}}{n}$} et
\(\OW{\fun{insl}}{n} = n\).\index{insl@$\OW{\fun{insl}}{n}$} Dans le
cas de l'insertion à la Andersson, à la \fig~\vref{fig:insl1}, le pire
des cas est identique mais il y a un appel supplémentaire pour
initialiser la clé candidate, d'où \(\W{\fun{insl}_1}{n} = n +
2\).\index{insl1@$\W{\fun{insl}_1}{n}$} De plus, le nombre de
comparaisons est symétrique et égale~\(1\) par n{\oe}ud interne, donc
\(\OW{\fun{insl}_1}{n} = n\)\index{insl1@$\OW{\fun{insl}_1}{n}$} et
tout arbre dégénéré est le pire des cas.

Le meilleur des cas pour l'insertion d'une feuille avec
\(\fun{insl/2}\) et \(\fun{insl\(_1\)/2}\) se produit quand la clé
doit devenir l'enfant, à gauche ou à droite, de la racine, ou, dit
autrement, la racine contient la clé minimum ou maximum dans le
parcours infixe, donc \(\B{\fun{insl}}{n} =
2\)\index{insl@$\B{\fun{insl}}{n}$} et \(\B{\fun{insl}_1}{n} =
3\).\index{insl@$\B{\fun{insl}_1}{n}$} En ce qui concerne les
comparaisons: \(\OB{\fun{insl}}{n} =
1\)\index{insl@$\OB{\fun{insl}}{n}$} et \(\OB{\fun{insl}_1}{n} =
2\).\index{insl1@$\OB{\fun{insl}_1}{n}$}

En examinant les extremums du coût de
\fun{mkl/1}\index{mkl@\fun{mkl/1}} et
\fun{mkr/1}\index{mkr@\fun{mkr/1}}, nous comprenons que nous ne
pouvons simplement ajouter les extremums du coût de \fun{insl/2} parce
que, comme nous l'avons mentionné plus tôt, l'appel
\(\fun{insl}(x,t)\) dépend de~\(x\) et de la forme de~\(t\). Par
exemple, après l'insertion de trois clés dans un arbre vide, la racine
n'a plus d'enfant vide, donc le meilleur des cas que nous avons
déterminé avant n'est plus valable.

Soit \(\OB{\fun{mkl}}{n}\)\index{mkl@$\OB{\fun{mkl}}{n}$} le nombre
minimum de comparaisons nécessaires pour cons\-trui\-re un arbre
binaire de recherche de taille~\(n\) par insertion de feuilles. Si
nous souhaitons minimiser le coût de chaque insertion, alors la
longueur interne de chaque n{\oe}ud doit être minimale et cela est
possible si l'arbre croît comme un arbre parfait\index{arbre
  binaire!$\sim$ parfait} ou presque parfait.\index{arbre
  binaire!$\sim$ presque parfait} Un arbre parfait est un arbre dont
les n{\oe}uds externes appartiennent au même niveau, une configuration
que nous avons déjà rencontrée \vpageref{par:perfection} (l'arbre est
contenu sans vide dans un triangle isocèle); un arbre presque parfait
est un arbre dont les n{\oe}uds externes sont sur deux niveaux
consécutifs et nous avons vu ce genre d'arbre au paragraphe consacré
aux arbres de comparaison et à la minimisation du coût moyen du tri
\vpageref{par:opt_sort_minimean}.

Supposons d'abord que l'arbre est parfait, de taille~\(n\) et
hauteur~\(h\).\index{arbre binaire!hauteur} La hauteur est la
longueur, comptée en nombre d'arcs, du chemin le plus long de la
racine à un n{\oe}ud externe. La longueur totale pour un niveau~\(k\)
constitué uniquement de n{\oe}uds internes est~\(k2^k\). Par
conséquent, en additionnant tous les niveaux, il vient
\begin{equation}
\OB{\fun{mkl}}{n} = \sum_{k=1}^{h-1}k2^k = (h-2)2^h + 2,
\label{eq_OBmkl_tmp1}
\end{equation}
grâce à l'équation~\eqref{eq_Sj} \vpageref{eq_Sj}. De plus, en
additionnant le nombre de n{\oe}uds internes par niveaux: \(n =
\sum_{k=0}^{h-1}2^k = 2^{h} - 1\), donc \(h = \lg(n+1)\), que nous
pouvons remplacer dans l'équation~\eqref{eq_OBmkl_tmp1} pour obtenir
enfin
\begin{equation*}
\OB{\fun{mkl}}{n} = (n+1)\lg(n+1) - 2n.
\end{equation*}
Nous avons prouvé \(1 + \floor{\lg n} = \ceiling{\lg(n+1)}\) en
déterminant le nombre maximum de comparaisons du tri par
interclassement descendant, à l'équation~\eqref{eq_top}
\vpageref{eq_top}, donc nous pouvons conclure:
\begin{equation}
\OB{\fun{mkl}}{n} = (n+1)\floor{\lg n} - n + 1.
\index{mkl@$\OB{\fun{mkl}}{n}$}
\label{eq_OBmkl_perfect}
\end{equation}

Supposons maintenant que l'arbre est presque parfait, avec
l'avant-dernier niveau \(h-1\) contenant \(q \neq 0\)~n{\oe}uds
internes; alors
\begin{equation}
\OB{\fun{mkl}}{n} = \sum_{k=1}^{h-2}k2^k + (h-1)q
= (h-3)2^{h-1} + 2 + (h-1)q.
\label{eq_OBmkl_tmp2}
\end{equation}
De plus, le nombre total~\(n\) de n{\oe}uds internes, lorsqu'il est le
résultat d'une somme par niveaux, satisfait \(n = \sum_{k=0}^{h-2}2^k
+ q = 2^{h-1} - 1 + q\), d'où \(q = n - 2^{h-1} + 1\). Par définition,
nous avons \(0 < q \leqslant 2^{h-1}\), donc \(0 < n - 2^{h-1} + 1
\leqslant 2^{h-1}\), d'où nous tirons \(h - 1 < \lg(n+1) \leqslant
h\), alors \(h = \ceiling{\lg(n+1)} = \floor{\lg n} + 1\), d'où \(q =
n - 2^{\floor{\lg n}} + 1\). Nous pouvons maintenant substituer à
\(h\)~et~\(q\) leur valeur, que nous venons de déterminer, en termes
de~\(n\) dans l'équation~\eqref{eq_OBmkl_tmp2}:
\begin{equation}
\OB{\fun{mkl}}{n} = (n+1)\floor{\lg n} - 2^{\floor{\lg n}} + 2.
\label{eq_OBmkl_almost_perfect}
\end{equation}
En comparant les équations~\eqref{eq_OBmkl_perfect}
et~\eqref{eq_OBmkl_almost_perfect}, nous voyons que le nombre de
comparaisons est minimisé si l'arbre est parfait, donc \(n = 2^p -
1\). L'approximation asymptotique de \(\OB{\fun{mkl}}{n}\) n'est pas
dure à trouver, du moment que nous évitons l'écueil \(2^{\floor{\lg
    n}} \sim n\). En effet, considérons la fonction \(x(p) := 2^p -
1\), où \(p\)~est un entier naturel. D'abord, remarquons que, pour
tout~\(p>0\),
\begin{equation*}
2^{p-1} \leqslant 2^p - 1 < 2^p \Rightarrow p-1 \leqslant \lg(2^p-1) <
p \Rightarrow \floor{\lg(2^p-1)} = p-1.
\end{equation*}
Par conséquent, \(2^{\floor{\lg(x(p))}} = 2^{p-1} = (x(p)+1)/2 \sim
x(p)/2 \nsim x(p)\), ce qui prouve que \(2^{\floor{\lg(n)}} \nsim n\)
lorsque \(n=2^p-1 \rightarrow \infty\). Au lieu de cela, dans le cas
de l'équation~\eqref{eq_OBmkl_perfect}, utilisons les inégalités
classiques \(x - 1 < \floor{x} \leqslant x\):
\begin{equation*}
(n+1)\lg n - 2n < \OB{\fun{mkl}}{n} \leqslant (n+1)\lg n - n + 1.
\end{equation*}
Dans le cas de l'équation~\eqref{eq_OBmkl_almost_perfect}, utilisons
la définition de la partie fractionnaire\index{partie fractionnaire}
\(\{x\} := x - \floor{x}\). Évidemment, \(0 \leqslant \{x\} <
1\). Alors
\begin{equation*}
\OB{\fun{mkl}}{n} = (n+1)\lg n - n \cdot \theta(\{\lg n\})
                    + 2 - \{\lg n\},
\end{equation*}
où \(\theta(x) := 1 + 2^{-x}\). Minimisons et maximisons le terme
linéaire: nous avons \(\min_{0 \leqslant x < 1}\theta(x) = \theta(1) =
3/2\) et \(\max_{0 \leqslant x < 1}\theta(x) = \theta(0) =
2\). En gardant à l'esprit que \(x=\{\lg n\}\), nous avons
\begin{equation*}
(n+1)\lg n - 2n + 2 < \OB{\fun{mkl}}{n} < (n+1)\lg n - \tfrac{3}{2}n + 1.
\end{equation*}
Dans tous les cas, il est clairement établi à ce point que
\(\OB{\fun{mkl}}{n} \sim n\lg n\).\index{mkl@$\OB{\fun{mkl}}{n}$}

Soit \(\OW{\fun{mkl}}{n}\)\index{mkl@$\OW{\fun{mkl}}{n}$} le nombre
maximum de comparaisons pour construire un arbre binaire de recherche
de taille~\(n\) par insertion de feuilles. Si nous maximisons chaque
insertion, nous devons faire croître un arbre dégénéré et insérer à un
n{\oe}ud de longueur externe maximale:
\begin{equation*}
\abovedisplayskip=4pt
\belowdisplayskip=0pt
\OW{\fun{mkl}}{n} =
\sum_{k=1}^{n-1}k = \frac{n(n-1)}{2} \sim \frac{1}{2}n^2.
\end{equation*}
\index{arbre binaire de recherche!insertion de feuilles|)}

\mypar{Insertion d'une racine}
\index{arbre binaire de recherche!insertion d'une racine|(}

Si des clés récemment insérées sont recherchées, le coût est
relativement élevé parce que ces clés sont des feuilles ou sont
proches de feuilles. Dans ce scénario, au lieu d'insérer une clé comme
une feuille, il est préférable de l'insérer comme une nouvelle racine
\citep{Stephenson_1980}. La méthode consiste à d'abord effectuer une
insertion de feuille et, sur le chemin du retour vers la racine
(c'est-à-dire, après l'évaluation de chaque appel récursif), nous
effectuons des rotations pour faire monter le nouveau n{\oe}ud dans
l'arbre. Plus précisément, si le n{\oe}ud a été inséré dans un
sous-arbre gauche, alors une rotation à droite l'amène un niveau plus
haut, sinon une rotation à gauche aura le même effet. La composition
de ces rotations fait monter la feuille jusqu'à la racine. La rotation
à droite, \fun{rotr/1}\index{rotr@\fun{rotr/1}} (anglais, \emph{rotate
  right}) et à gauche, \fun{rotl/1}\index{rotl@\fun{rotl/1}} (anglais,
\emph{rotate left}),\index{arbre binaire!rotation} ont été discutées à
la section~\ref{sec:traversals} \vpageref{par:rotation} et sont
définies à la \fig~\vref{fig:rotations}.
\begin{figure}[b]
\begin{equation*}
\boxed{
\begin{array}{r@{\;}l@{\;}l}
\fun{rotr}(\fun{bst}(y,\fun{bst}(x,t_1,t_2),t_3))
& \xrightarrow{\smash{\epsilon}} & \fun{bst}(x,t_1,\fun{bst}(y,t_2,t_3)).\\
\fun{rotl}(\fun{bst}(x,t_1,\fun{bst}(y,t_2,t_3)))
& \xrightarrow{\smash{\zeta}} & \fun{bst}(y,\fun{bst}(x,t_1,t_2),t_3).
\end{array}}
\end{equation*}
\caption{Rotation à droite (\(\smash{\epsilon}\)) et à gauche (\(\smash{\zeta}\))\label{fig:rotations}}
\end{figure}
Clairement, elles commutent et sont inverses l'une de l'autre:
\begin{equation*}
\abovedisplayskip=5pt
\belowdisplayskip=5pt
\fun{rotl}(\fun{rotr}(t)) \equiv \fun{rotr}(\fun{rotl}(t)) \equiv t.
\end{equation*}
De plus, bien que moins évident, elle préservent les parcours infixes:
\begin{equation*}
  \abovedisplayskip=5pt
  \belowdisplayskip=5pt
  \fun{in}_3(\fun{rotl}(t)) \equiv \fun{in}_3(\fun{rotr}(t)) \equiv
  \fun{in}_3(t),
\end{equation*}
où \fun{in\(_3\)/1}\index{in3@\fun{in\(_3\)/1}} calcule le parcours
infixe d'un arbre: \(\fun{in}_3(t) \rightarrow \fun{in}_2(t,\el)\), et
\fun{in\(_2\)/2}\index{in2@\fun{in\(_2\)/2}} est définie à la
\fig~\vref{fig:bst0}. Ce théorème est relié de façon inhérente à
\(\pred{Rot}{x,y,t_1,t_2,t_3}\)\index{Rot@\predName{Rot}},
\vpageref{def_Rot}, et il est facile à démontrer, sans recours à
l'induction. D'abord, nous remarquons que si \(\fun{in}_3(t) \equiv
\fun{in}_3(\fun{rotl}(t))\), remplacer \(t\) par \(\fun{rotr}(t)\)
donne \(\fun{in}_3(\fun{rotr}(t)) \equiv
\fun{in}_3(\fun{rotl}(\fun{rotr}(t))) \equiv \fun{in}_3(t)\), donc
nous n'avons besoin de prouver que \(\fun{in}_3(\fun{rotl}(t)) \equiv
\fun{in}_3(t)\). Puisque le membre gauche est plus grand, nous
devrions essayer de le réécrire en le membre droit. Les réécritures à
la \fig~\vref{fig:in_rotr} supposent une rotation gauche, donc l'arbre
a la forme \(t=\fun{bst}(x,t_1,\fun{bst}(y,t_2,t_3))\).
\begin{figure}
  \begin{equation*}
    \boxed{
      \begin{array}{@{}r@{\;}l@{\;}l@{}}
  \fun{in}_3(\fun{rotl}(t))
  & \rightarrow & \fun{in}_2(\fun{rotl}(t),\el)\\
  & = & \fun{in}_2(\fun{rotl}(\fun{bst}(x,t_1,\fun{bst}(y,t_2,t_3))),\el)\\
  & \xrightarrow{\smash{\epsilon}} &
  \fun{in}_2(\fun{bst}(y,\fun{bst}(x,t_1,t_2),t_3),\el)\\
  & \rightarrow &
  \fun{in}_2(\fun{bst}(x,t_1,t_2),\cons{y}{\fun{in}_2(t_3,\el)})\\
  & \Rrightarrow &
 \fun{in}_2(t_1,\cons{x}{\fun{in}_2(t_2,\cons{y}{\fun{in}_2(t_3,\el)})})\\
  & \leftarrow &
  \fun{in}_2(t_1,\cons{x}{\fun{in}_2(\fun{bst}(y,t_2,t_3),\el)})\\
  & \leftarrow &
  \fun{in}_2(\fun{bst}(x,t_1,\fun{bst}(y,t_2,t_3)),\el)\\
  & = &
  \fun{in}_2(t,\el)\\
  & \leftarrow &
  \fun{in}_3(t).\hfill\Box
\end{array}
}
\end{equation*}
\caption{Preuve de \(\fun{in}_3(\fun{rotl}(t)) \equiv
  \fun{in}_3(t)\)\label{fig:in_rotr}}
\end{figure}
Si nous appliquons une rotation à des sous-arbres, comme nous l'avons
fait, par exemple, à la \fig~\vref{fig:rot}, le même théorème implique
que le parcours infixe de l'arbre en entier est invariant.

Un corollaire est que les rotations laissent inchangée la propriété
d'être un arbre binaire de recherche (\fig~\vref{fig:bst}):
\begin{equation*}
\abovedisplayskip=5pt
\belowdisplayskip=5pt
\fun{bst}(\fun{rotl}(t)) \equiv \fun{bst}(\fun{rotr}(t)) \equiv \fun{bst}(t).
\end{equation*}
En effet, en supposant que \fun{bst/1} est la spécification de
\fun{bst\(_0\)/1} à la \fig~\vref{fig:bst0}, et que celle-ci est
correcte, à savoir \(\fun{bst}(t) \equiv \fun{bst}_0(t)\), il est
assez aisé de démontrer notre théorème, avec l'aide du théorème
précédent \(\fun{in}_3(\fun{rotl}(t)) \equiv \fun{in}_3(t)\), qui
équivaut à \(\fun{in}_2(\fun{rotl}(t),\el) \equiv \fun{in}_2(t,\el)\),
et en remarquant qu'il est suffisant de prouver
\(\fun{bst}_0(\fun{rotl}(t)) \equiv \fun{bst}_0(t)\). Nous concluons
alors:
\begin{equation*}
\fun{bst}_0(\fun{rotl}(t))
\Rrightarrow \fun{ord}(\fun{in}_2(\fun{rotl}(t),\el))
\equiv \fun{ord}(\fun{in}_2(t,\el))
\leftarrow \fun{bst}_0(t).
\end{equation*}

Considérons maintenant un exemple d'insertion d'une racine à la
\fig~\vref{fig:insr_ex}, où l'arbre de la \fig~\vref{fig:bst_ex} est
augmenté avec~\(7\).
\begin{figure}
\centering
\includegraphics[bb=71 641 406 724]{insr_ex}%[bb=71 645 406 718]
\caption{Insertion de la racine \(7\) dans la \fig~\vref{fig:bst_ex}
\label{fig:insr_ex}}
\end{figure}
Remarquons que la clôture transitive \((\twoheadrightarrow)\) capture
l'insertion de feuille préliminaire, que
\((\xrightarrow{\smash{\epsilon}})\) est une rotation à droite et que
\((\xrightarrow{\smash{\zeta}})\) est une rotation à gauche. Il ne
s'agit maintenant que de simplement modifier la définition de
\fun{insl/2} pour qu'elle devienne l'insertion de racine sous le nom
de \fun{insr/2}, à la \fig~\vref{fig:insr}.
\begin{figure}
\begin{equation*}
\boxed{
\begin{array}{r@{\;}l@{\;}l}
\fun{insr}(y,\fun{bst}(x,t_1,t_2)) & \xrightarrow{\smash{\eta}} &
  \fun{rotr}(\fun{bst}(x,\fun{insr}(y,t_1),t_2)),
  \; \text{si \(x \succ y\)};\\
\fun{insr}(y,\fun{bst}(x,t_1,t_2)) & \xrightarrow{\smash{\theta}} &
  \fun{rotl}(\fun{bst}(x,t_1,\fun{insr}(y,t_2)));\\
\fun{insr}(y,\fun{ext}()) & \xrightarrow{\smash{\iota}} & \fun{bst}(y,\fun{ext}(),\fun{ext}()).
\end{array}}
\end{equation*}
\caption{Insertion de racine avec doublons possibles\label{fig:insr}}
\end{figure}
Notons que nous pouvons éviter de créer les n{\oe}uds internes
temporaires \(\fun{bst}(x,\dots,t_2)\) et \(\fun{bst}(x,t_1,\dots)\)
en modifiant \fun{rotl/1} et \fun{rotr/1} de telle sorte qu'elles
prennent trois arguments (\fun{rotl\(_0\)/3} et \fun{rotr\(_0\)/3}),
comme on le voit avec la nouvelle version \fun{insr\(_0\)/2} à la
\fig~\ref{fig:insr0}.
\begin{figure}[b]
\begin{equation*}
\boxed{
\begin{array}{r@{\;}l@{\;}l}
\fun{insr}_0(y,\fun{bst}(x,t_1,t_2)) & \rightarrow &
  \fun{rotr}_0(x,\fun{insr}_0(y,t_1),t_2),
  \; \text{si \(x \succ y\)};\\
\fun{insr}_0(y,\fun{bst}(x,t_1,t_2)) & \rightarrow &
  \fun{rotl}_0(x,t_1,\fun{insr}_0(y,t_2));\\
\fun{insr}_0(y,\fun{ext}()) & \rightarrow &
\fun{bst}(y,\fun{ext}(),\fun{ext}()).\\
\\
\fun{rotr}_0(y,\fun{bst}(x,t_1,t_2),t_3)
& \rightarrow & \fun{bst}(x,t_1,\fun{bst}(y,t_2,t_3)).\\
\fun{rotl}_0(x,t_1,\fun{bst}(y,t_2,t_3))
& \rightarrow & \fun{bst}(y,\fun{bst}(x,t_1,t_2),t_3).
\end{array}}
\end{equation*}
\caption{Insertion de racine avec doublons possibles (bis)\label{fig:insr0}}
\end{figure}

Une comparaison entre les insertions de feuille et de racine révèle
des faits intéressants. Par exemple, puisque l'insertion de feuilles
ne déplace aucun n{\oe}ud, les constructions d'un même arbre à partir
de deux permutations des clés ont le même coût, par exemple
\((1,3,2,4)\) et \((1,3,4,2)\). D'un autre côté, comme l'ont remarqué
\cite{GeldenhuysVanderMerwe_2009}, fabriquer le même arbre de
recherche avec des insertions de racine peut mener à des coûts
différents, comme \((1,2,4,3)\) et \((1,4,2,3)\). Ces auteurs aussi
prouvent que tous les arbres d'une taille donnée peuvent être obtenus
par insertion de racines ou de feuilles parce que
\begin{equation}
\pred{RootLeaf}{s} \colon \fun{mkr}(s) \equiv \fun{mkl}(\fun{rev}(s)),
\label{thm_RootLeaf}
\index{RootLeaf@\predName{RootLeaf}}
\end{equation}
où \fun{mkr/1}\index{mkr@\fun{mkr/1}} \index{mkr@\fun{mkr/2}}
(anglais, \emph{make roots}) est aisément définie à la
\fig~\vref{fig:mkr}.
\begin{figure}
\begin{equation*}
\boxed{
\begin{array}{@{}r@{\;}l@{\;}lr@{\;}l@{\;}l@{}}
\fun{mkr}(s) & \xrightarrow{\smash{\kappa}} &
\fun{mkr}(s,\fun{ext}()).
& \fun{mkr}(\el,t) & \xrightarrow{\smash{\lambda}} & t;\\
&&&\fun{mkr}(\cons{x}{s},t) & \xrightarrow{\smash{\mu}} & \fun{mkr}(s,\fun{insr}(x,t)).
\end{array}}
\end{equation*}
\caption{Créer des arbres avec des insertions de racines
\label{fig:mkr}}
\end{figure}
Remarquons que cela équivaut à affirmer \(\fun{mkr}(s) \equiv
\fun{mklR}(s)\), où \fun{mklR/1} est définie par
l'équation~\eqref{eq_mklR} \vpageref{eq_mklR}. Il est utile de
démontrer ici \(\pred{RootLeaf}{s}\) parce que, contrairement à
\cite{GeldenhuysVanderMerwe_2009}, nous voulons employer une induction
structurelle pour être guidé exactement par la syntaxe des définitions
de fonction, plutôt qu'une induction sur les tailles, un concept
adventice, et nous voulons éviter aussi d'user d'ellipses dans la
description des données. De plus, notre cadre logique n'est pas séparé
de nos définitions de fonction (le programme abstrait): les
réécritures elles-mêmes, les étapes de calcul, donnent naissance à
une interprétation logique en termes de classes d'équivalences.

Nous commençons par remarquer que \(\pred{RootLeaf}{s}\) équivaut à
\begin{equation*}
\pred{RootLeaf\(_0\)}{s} \colon \fun{mkr}(s) \equiv
  \fun{mkl}(\fun{rev}_0(s)),
\index{RootLeaf0@\predName{RootLeaf\(_0\)}}
\end{equation*}
où \fun{rev\(_0\)/1} est définie au début de la
section~\vref{sec:reversal}, où nous prouvons \(\pred{EqRev}{s} \colon
\fun{rev}_0(s) \equiv \fun{rev}(s)\).\index{EqRev@\predName{EqRev}}
C'est souvent une bonne idée d'employer \fun{rev\(_0\)/1} dans des
preuves inductives parce que la règle~\(\smash{\delta}\) définit
\(\fun{rev}_0(\cons{x}{s})\) directement en termes de
\(\fun{rev}_0(s)\). Rappelons les définitions concernées:
\begin{equation*}
  \begin{array}{@{}r@{\;}l@{\;}lr@{\;}l@{\;}l@{}}
  \fun{cat}(\el,t)\index{cat@\fun{cat/2}}
& \xrightarrow{\smash{\alpha}} & t;
& \fun{rev}_0(\el)
& \xrightarrow{\smash{\gamma}} & \el;\\
  \fun{cat}(\cons{x}{s},t)
& \xrightarrow{\smash{\beta}} & \cons{x}{\fun{cat}(s,t)}.
& \fun{rev}_0(\cons{x}{s})
& \xrightarrow{\smash{\delta}} & \fun{cat}(\fun{rev}_0(s),[x]).
\end{array}
\end{equation*}
Bien sûr, \fun{rev\(_0\)/1}\index{rev0@\fun{rev\(_0\)/1}} est inutile
en tant que programme à cause de son coût quadratique, qui ne peut
battre le coût linéaire de \fun{rev/1}\index{rev@\fun{rev/1}}, mais,
en ce qui concerne la preuve de théorème, elle est une spécification
valable et le lemme \(\pred{EqRev}{s}\) nous permet de transférer
toute équivalence impliquant \fun{rev\(_0\)/1} en une équivalence avec
\fun{rev/1}.

Procédons par induction sur la structure de la pile~\(s\). D'abord,
nous avons besoin de prouver directement (sans induction)
\(\pred{RootLeaf\(_0\)}{\el}\):
\begin{equation*}
\fun{mkr}(\el) \!\xrightarrow{\smash{\kappa}}\! \fun{mkr}(\el,\fun{ext}())
\!\xrightarrow{\smash{\lambda}}\! \fun{ext}()
\!\xleftarrow{\smash{\psi}}\! \fun{mkl}(\el,\fun{ext}())
\!\xleftarrow{\smash{\xi}}\! \fun{mkl}(\el)
\!\xleftarrow{\smash{\gamma}}\! \fun{mkl}(\fun{rev}_0(\el)\!).
\end{equation*}
Ensuite, nous posons l'hypothèse d'induction
\(\pred{RootLeaf\(_0\)}{s}\) et nous passons à la preuve de
\(\pred{RootLeaf\(_0\)}{\cons{x}{s}}\), pour tout~\(x\). Étant donné
que le membre droit est plus grand, nous commençons par le réécrire et
si nous nous sentons fourvoyés, nous réécrivons l'autre membre, en
visant leur convergence. En chemin, il y aura des étapes, sous la
forme d'équivalences, qui constituent des lemmes (sous-buts) qui
devront être démontrés ultérieurement. Nous avons:
\begin{equation*}
\begin{array}{@{}r@{\;}l@{\;}l@{\qquad}r@{}}
\fun{mkl}(\fun{rev}_0(\cons{x}{s}))
& \xrightarrow{\smash{\delta}} &
  \fun{mkl}(\fun{cat}(\fun{rev}_0(s),[x]))\\
& \Rra{\xi} &
  \fun{mkl}(\fun{cat}(\fun{rev}_0(s),[x]),\fun{ext}())\\
& \equiv_0 & \fun{mkl}([x],\fun{mkl}(\fun{rev}_0(s),\fun{ext}()))
& (\text{Lemme})\\
& \Rra{\omega} &
  \fun{mkl}(\el,\fun{insl}(x,\fun{mkl}(\fun{rev}_0(s),\fun{ext}())))\\
& \Rra{\psi} &
  \fun{insl}(x,\fun{mkl}(\fun{rev}_0(s),\fun{ext}()))\\
& \Lla{\xi} &
  \fun{insl}(x,\fun{mkl}(\fun{rev}_0(s)))\\
& \equiv & \fun{insl}(x,\fun{mkr}(s)) & (\pred{RootLeaf\(_0\)}{s}\!)\\
& \xrightarrow{\smash{\xi}} &
  \fun{insl}(x,\fun{mkr}(s,\fun{ext}()))\\
& \equiv_1 & \fun{mkr}(s,\fun{insl}(x,\fun{ext}()))
& (\text{Lemme})\\
& \xrightarrow{\smash{\phi}} &
  \fun{mkr}(s,\fun{bst}(x,\fun{ext}(),\fun{ext}()))\\
& \xleftarrow{\smash{\iota}} &
  \fun{mkr}(s,\fun{insr}(x,\fun{ext}()))\\
& \xleftarrow{\smash{\mu}} & \fun{mkr}(\cons{x}{s},\fun{ext}())\\
& \xleftarrow{\smash{\kappa}} & \fun{mkr}(\cons{x}{s}).
& \hfill\Box
\end{array}
\end{equation*}

Maintenant, nous devons démontrer les deux lemmes que nous avons
identifiés lors de notre tentative de preuve. Le premier, dénoté par
\((\equiv_0)\), est un corollaire de \index{MklCat@\predName{MklCat}}
\(\pred{MklCat}{u,v,t} \colon \fun{mkl}(\fun{cat}(u,v),t) \equiv_0
\fun{mkl}(v,\fun{mkl}(u,t))\). La première chose à faire face à une
nouvelle proposition est d'essayer de la réfuter en choisissant
astucieusement des valeurs de ses variables. Dans ce cas, la vérité de
ce lemme peut être intuitivement saisie sans efforts, ce qui nous
donne confiance pour rechercher une preuve formelle, plutôt que de
faire sans. Il est suffisant de raisonner par induction sur la
structure de la pile~\(u\). D'abord, nous vérifions
\(\pred{MklCat}{\el,v,t}\):
\begin{equation*}
\fun{mkl}(\fun{cat}(\el,v),t)
  \xrightarrow{\smash{\alpha}} \fun{mkl}(v,t)
  \xleftarrow{\smash{\psi}} \fun{mkl}(v,\fun{mkl}(\el,t)).
\end{equation*}
Ensuite, nous supposons \(\pred{MklCat}{u,v,t}\), pour tout~\(v\)
et~\(t\), qui est donc l'hypothèse d'induction, et nous prouvons
\(\pred{MklCat}{\cons{x}{u},v,t}\):
\begin{equation*}
  \begin{array}{@{}r@{\;}l@{\;}l@{\quad}r@{}}
      \fun{mkl}(\fun{cat}(\cons{x}{u},v),t)
& \xrightarrow{\smash{\beta}} &
  \fun{mkl}(\cons{x}{\fun{cat}(u,v)},t)\\
& \Rra{\omega} &
  \fun{mkl}(\fun{cat}(u,v),\fun{insl}(x,t))\\
& \equiv_0 & \fun{mkl}(v,\fun{mkl}(u,\fun{insl}(x,t)))
         & \!\!(\pred{MklCat}{u,v,\fun{inst}(x,t)}\!)\\
& \xleftarrow{\smash{\omega}} &
  \fun{mkl}(v,\fun{mkl}(\cons{x}{u},t)). & \hfill\Box
\end{array}
\end{equation*}

Définissons formellement le second lemme dénoté par l'équivalence
\((\equiv_1)\) dans la preuve de \(\pred{RootLeaf\(_0\)}{s}\). Soit
\begin{equation*}
  \pred{MkrInsr}{x,s,t} \colon
  \fun{insl}(x,\fun{mkr}(s,t)) \equiv_1 \fun{mkr}(s,\fun{insl}(x,t)).
\end{equation*}
Cette proposition, malgré sa symétrie symbolique plaisante, n'est pas
triviale et peut exiger quelques exemples pour être mieux saisie. Elle
signifie que l'insertion d'une feuille peut être effectuée avant ou
après une série d'insertion de racines, aboutissant dans les deux cas
au même arbre. Nous construisons la preuve par induction sur la
structure de la pile~\(s\) uniquement. (Les autres paramètres ne sont
probablement pas inductivement pertinents parce que \(x\)~est une clé,
donc nous ne pouvons rien dire sur son éventuelle structure interne,
et \(t\)~est le second paramètre de \fun{mkr/2}\index{mkr@\fun{mkr/2}}
et \fun{insl/2}\index{insl@\fun{insl/2}}, donc nous ne savons rien de
sa forme ou contenu.) Nous débutons, comme il se doit, par une
vérification (Une vérification, par définition, n'implique pas l'usage
d'un argument inductif.) de la base\index{MkInsr@\predName{MkInsr}}
\(\pred{MkInsr}{x,\el,t}\):
\begin{equation*}
  \fun{insl}(x,\fun{mkr}(\el,t))
\xrightarrow{\smash{\lambda}} \fun{insl}(x,t)
\Lla{\lambda} \fun{mkr}(\el,\fun{insl}(x,t)).
\end{equation*}
Nous supposons maintenant \(\pred{MkrInsr}{x,s,t}\) pour tout~\(x\)
et~\(t\), et nous tâchons de démontrer
\(\pred{MkrInsr}{x,\cons{y}{s},t}\), pour toute clé~\(y\), en
réécrivant les deux membres de l'équivalence et en visant à obtenir le
même terme:
\begin{equation*}
\begin{array}{@{}r@{\;}l@{\;}l@{\;\;\,}r@{}}
  \fun{insl}(x,\fun{mkr}(\cons{y}{s},t))
& \xrightarrow{\smash{\mu}} &
  \fun{insl}(x,\fun{mkr}(s,\fun{insr}(y,t)))\\
& \equiv_1 & \fun{mkr}(s,\fun{insl}(x,\fun{insr}(y,t)))
           & (\pred{MkrInsr}{x,s,\fun{insr}(y,t)}\!)\\
& \equiv_2 & \fun{mkr}(s,\fun{insr}(y,\fun{insl}(x,t)))
           & (\text{Lemme})\\
& \Lla{\mu} &
  \fun{mkr}(\cons{y}{s},\fun{insl}(x,t)).
& \hfill\Box
\end{array}
\end{equation*}
Remarquons que nous avons découvert la nécessité d'un nouveau lemme
sous la forme de l'équivalence \((\equiv_2)\), qui affirme que
l'insertion d'une racine commute avec l'insertion d'une feuille. Ceci
n'est pas évident et nécessite probablement d'être vu sur quelques
exemples pour être cru. La technique de démonstration inductive
elle-même nous a conduit à cet important concept sur lequel repose
donc la proposition initiale. Soit le lemme en question formellement
défini comme suit:
\begin{equation*}
\pred{Ins}{x,y,t} \colon \fun{insl}(x,\fun{insr}(y,t))
\equiv_2 \fun{insr}(y,\fun{insl}(x,t)).
\index{Ins@\predName{Ins}}
\end{equation*}
Nous utiliserons l'induction sur la structure de l'arbre~\(t\), parce
que les autres variables sont des clés, donc sont atomiques. La
vérification de la base \(\pred{Ins}{x,y,\fun{ext}()}\) est en fait
plutôt longue, en comparaison avec les autres preuves. Nous
commençons avec
\begin{equation*}
\begin{array}{@{}r@{\;}l@{\;}l@{\quad}r@{}}
  \fun{insl}(x,\fun{insr}(y,\fun{ext}()))
& \xrightarrow{\smash{\iota}} &
  \fun{insl}(x,\fun{bst}(y,\fun{ext}(),\fun{ext}())) & \otimes
\end{array}
\end{equation*}
Le symbole \(\otimes\) est une étiquette à partir de laquelle
différentes réécritures sont possibles, selon différentes conditions,
et nous aurons besoin de revenir à cette étiquette. Ici, deux cas se
présentent d'eux-même à nous: \(x \succ y\) ou \(y \succ x\). Nous
avons l'analyse par cas suivante:
\begin{itemize}

  \item Si \(x \succ y\), alors
\begin{equation*}
\begin{array}{@{}r@{\;}l@{\;}l@{\quad}r@{}}
\otimes
& \xrightarrow{\smash{\upsilon}} &
  \fun{bst}(y,\fun{ext}(),\fun{insl}(x,\fun{ext}()))
& (x \succ y)\\
& \xrightarrow{\smash{\phi}} &
  \fun{bst}(y,\fun{ext}(),\fun{bst}(x,\fun{ext}(),\fun{ext}()))\\
& \xleftarrow{\smash{\epsilon}} &
\fun{rotr}(\fun{bst}(x,\fun{bst}(y,\fun{ext}(),\fun{ext}()),\fun{ext}()))\\
& \xleftarrow{\smash{\iota}} &
  \fun{rotr}(\fun{bst}(x,\fun{insr}(y,\fun{ext}()),\fun{ext}()))\\
& \xleftarrow{\smash{\eta}} &
  \fun{insr}(y,\fun{bst}(x,\fun{ext}(),\fun{ext}()))
& (x \succ y)\\
& \xleftarrow{\smash{\phi}} &
  \fun{insr}(y,\fun{insl}(x,\fun{ext}())).
\end{array}
\end{equation*}

  \item Si \(y \succ x\), alors
\begin{equation*}
\begin{array}{@{}r@{\;}l@{\;}l@{\qquad}r@{}}
\otimes & \xrightarrow{\smash{\tau}} &
  \fun{bst}(y,\fun{insl}(x,\fun{ext}()),\fun{ext}())
& (y \succ x)\\
& \xrightarrow{\smash{\phi}} &
  \fun{bst}(y,\fun{bst}(x,\fun{ext}(),\fun{ext}()),\fun{ext}())\\
& \xleftarrow{\smash{\xi}} &
\fun{rotl}(\fun{bst}(x,\fun{ext}(),\fun{bst}(y,\fun{ext}(),\fun{ext}())))\\
& \xleftarrow{\smash{\iota}} &
  \fun{rotl}(\fun{bst}(x,\fun{ext}(),\fun{insr}(y,\fun{ext}())))\\
& \xleftarrow{\smash{\theta}} &
  \fun{insr}(y,\fun{bst}(x,\fun{ext}(),\fun{ext}()))
& (y \succ x)\\
& \xleftarrow{\smash{\phi}} &
  \fun{insr}(y,\fun{insl}(x,\fun{ext}())).
\end{array}
\end{equation*}
\end{itemize}

Maintenant, supposons \(\pred{Ins}{x,y,t_1}\) et
\(\pred{Ins}{x,y,t_2}\), puis essayons de prouver
\(\pred{Ins}{x,y,t}\), où \(t=\fun{bst}(a,t_1,t_2)\), pour toute
clé~\(a\). Nous débutons arbitrairement avec le membre droit:
\begin{equation*}
\begin{array}{@{}r@{\;}l@{\;}l@{\quad}r@{}}
  \fun{insr}(y,\fun{insl}(x,t))
& = & \fun{insr}(y,\fun{insl}(x,\fun{bst}(a,t_1,t_2))) & \otimes
\end{array}
\end{equation*}
Deux cas complémentaires font surface: \(a \succ x\) ou \(x \succ a\).
\begin{itemize}

  \item Si \(a \succ x\), alors \(\otimes \xrightarrow{\smash{\tau}}
  \fun{insr}(y,\fun{bst}(a,\fun{insl}(x,t_1),t_2)) \; \otimes\). Deux
  sous-cas apparaissent: \(a \succ y\) ou \(y \succ a\).
  \begin{itemize}

    \item Si \(a \succ y\), alors
      \begin{equation*}
      \begin{array}{@{}r@{\;}l@{\;}l@{\hspace{-8mm}}r@{}}
        \otimes & \Rra{\eta} &
        \fun{rotr}(\fun{bst}(a,\fun{insr}(y,\fun{insl}(x,t_1)),t_2))
        & (a \succ y)\\
        & \equiv_2 & \fun{rotr}(\fun{bst}(a,\fun{insl}(x,
        \fun{insr}(y,t_1)),t_2)) & (\pred{Ins}{x,y,t_1})\\
        & \Lla{\tau} & \fun{rotr}(\fun{insl}(x,
        \fun{bst}(a,\fun{insr}(y,t_1),t_2)))\\
        & \equiv & \fun{rotr}(\fun{insl}(x,
        \fun{rotl}(\fun{rotr}(\fun{bst}(a,\fun{insr}(y,t_1),t_2)))))\\
        & \xleftarrow{\smash{\eta}} &
        \fun{rotr}(\fun{insl}(x,\fun{rotl}(\fun{insr}(y,
        \fun{bst}(a,t_1,t_2)))))\\
        & = & \fun{rotr}(\fun{insl}(x,\fun{rotl}(\fun{insr}(y,t))))
        & (t=\fun{bst}(a,t_1,t_2))\\
        & \equiv_3 & \fun{rotr}(\fun{rotl}(\fun{insl}(x,
        \fun{insr}(y,t))))) & (\text{Lemme})\\
        & \equiv & \fun{insl}(x,\fun{insr}(y,t)).
        & (\fun{rotr}(\fun{rotl}(z)) \equiv z)
      \end{array}
      \end{equation*}
      Ce qui fait marcher ce cas de la preuve est que \(a \succ x\) et
      \(a \succ y\) nous permettent de déplacer les appels aux
      rotations dans le terme de telle sorte qu'ils sont composés sur
      le sous-arbre~\(t_1\), ce qui rend possible l'usage de
      l'hypothèse d'induction \(\pred{Ins}{x,y,t_1}\). Ensuite, nous
      ramenons vers le haut du terme les appels commutés, en utilisant
      le fait que la composition d'une rotation à gauche et à droite,
      et vice-versa, est l'identité. Remarquons comment, en cours de
      route, nous avons mis au jour une nouvelle équivalence,
      \((\equiv_3)\), en attente de démonstration. L'interprétation de
      ce sous-but est que la rotation à gauche et l'insertion d'une
      feuille commutent, éclairant davantage le sujet.

    \item Si \(y \succ a\), alors
\begin{equation*}
\begin{array}{@{}r@{\;}l@{\;}l@{\hspace{-8mm}}r@{}}
  \otimes & \xrightarrow{\smash{\theta}} &
  \fun{rotl}(\fun{bst}(a,\fun{insl}(x,t_1),\fun{insr}(y,t_2)))
  & (y \succ a)\\
  & \Lla{\tau} &
  \fun{rotl}(\fun{insl}(x,\fun{bst}(a,t_1,\fun{insr}(y,t_2)))\\
  & \equiv &
  \fun{rotl}(\fun{insl}(x,\fun{rotr}(\fun{rotl}(\fun{bst}(a,t_1,
  \fun{insr}(y,t_2))))))\\
  & \xleftarrow{\smash{\theta}} &
  \fun{rotl}(\fun{insl}(x,\fun{rotr}(\fun{insr}(y,
  \fun{bst}(a,t_1,t_2)))))\\
  & = & \fun{rotl}(\fun{insl}(x,\fun{rotr}(\fun{insr}(y,t))))
  & (t=\fun{bst}(a,t_1,t_2))\\
  & \equiv_4 & \fun{rotl}(\fun{rotr}(\fun{insl}(x,\fun{insr}(y,t))))
  & (\text{Lemme})\\
  & \equiv & \fun{insl}(x,\fun{insr}(y,t))).
  & (\fun{rotl}(\fun{rotr}(z)) \equiv z)
\end{array}
\end{equation*}
Ici, il n'y avait pas besoin de l'hypothèse d'induction, parce que \(a
\succ x\) et \(y \succ a\) impliquent \(y \succ x\), donc l'insertion
de feuille et l'insertion de racine ne sont pas composées et
s'appliquent à des sous-arbres différents, \(t_1\) et~\(t_2\). Tout ce
que nous avons à faire est alors de les faire remonter dans le terme
dans le même ordre que nous les avons fait descendre (comme via une
file d'attente). Nous avons découvert au passage une nouvelle
équivalence, \((\equiv_4)\), qui nécessitera une preuve ultérieure et
qui est reliée par dualité à \((\equiv_3)\) parce qu'elle dit qu'une
rotation à \emph{droite} et une insertion de feuille
commutent. Ensemble, elles signifient qu'une rotation commute avec
l'insertion d'une feuille.
  \end{itemize}

\item Si \(x \succ a\), alors \(\otimes \xrightarrow{\smash{\upsilon}}
  \fun{insr}(y, \fun{bst}(a, t_1, \fun{insl}(y,t_2))) \;
  \otimes\). Deux sous-cas deviennent apparents: \(a \succ y\) ou \(y
  \succ a\).
  \begin{itemize}

    \item Si \(a \succ y\), alors
\begin{equation*}
\begin{array}{@{}r@{\;}l@{\;}l@{\hspace{-9mm}}r@{}}
  \otimes & \Rra{\eta} & \fun{rotr}(\fun{bst}(a,
  \fun{bst}(a,\fun{insr}(y,t_1),\fun{insl}(x,t_2))))
  & (a \succ y)\\
  & \Lla{\upsilon} &
  \fun{rotr}(\fun{insl}(x,\fun{bst}(a,\fun{insr}(y,t_1),t_2)))\\
  & \equiv & \fun{rotr}(\fun{insl}(x,
  \fun{rotl}(\fun{rotr}(\fun{bst}(a,\fun{insr}(y,t_1),t_2)))))\\
  & \xleftarrow{\smash{\eta}} &
  \fun{rotr}(\fun{insl}(x,\fun{rotl}(\fun{insr}(y,
  \fun{bst}(a,t_1,t_2)))))\\
  & = & \fun{rotr}(\fun{insl}(x,\fun{rotl}(\fun{insr}(y,t))))
  & (t=\fun{bst}(a,t_1,t_2))\\
  & \equiv_3 &
  \fun{rotr}(\fun{rotl}(\fun{insl}(x,\fun{insr}(y,t))))\\
  & \equiv & \fun{insl}(x,\fun{insr}(y,t)).
  & (\fun{rotr}(\fun{rotl}(z)) \equiv z)
\end{array}
\end{equation*}
Ce sous-cas est similaire au précédent, dans le sens que les
insertions sont effectuées sur des sous-arbres différents, donc il n'y
a pas besoin de l'hypothèse d'induction. La différence est qu'ici
\((\equiv_3)\) est exigée au lieu de \((\equiv_4)\).

  \item Si \(y \succ a\), alors
\begin{equation*}
\begin{array}{@{}r@{\;}l@{\;}l@{\qquad}r@{}}
  \otimes & \Rra{\theta} &
  \fun{rotl}(\fun{bst}(a,t_1,\fun{insr}(y,\fun{insl}(x,t_2))))\\
  & \equiv_2 & \fun{rotl}(\fun{bst}(a,t_1,\fun{insl}(x,
  \fun{insr}(y,t_2)))) & (\pred{Ins}{x,y,t_2})\\
  & \Lla{\upsilon} &
  \fun{rotl}(\fun{insl}(x,\fun{bst}(a,t_1,\fun{insr}(y,t_2))))\\
  & \equiv_3 & \fun{insl}(x,\fun{rotl}(\fun{bst}(a,t_1,
  \fun{insr}(y,t_2))))\\
  & \xleftarrow{\smash{\theta}} & \fun{insl}(x,\fun{insr}(y,
  \fun{bst}(a,t_1,t_2)))\\
  & = & \fun{insl}(x,\fun{insr}(y,t)). & (t=\fun{bst}(a,t_1,t_2))
\end{array}
\end{equation*}
Ceci est le dernier sous-cas. Il est similaire au premier, parce que
les insertions sont composées, bien que sur~\(t_2\) au lieu
de~\(t_1\), donc permettent l'usage de l'hypothèse
d'induction. Ensuite, les insertions sont remontées dans le même ordre
où elles ont descendu, par exemple, \fun{insl/2} a été poussé vers le
bas avant \fun{insr/2} donc son appel est remonté avant celui de
\fun{insr/2}.\hfill\(\Box\)
  \end{itemize}

\end{itemize}

\bigskip

Maintenant, nous avons deux lemmes à prouver, duaux l'un de l'autre,
et qui signifient ensemble que les rotations commutent avec
les insertions de feuilles. Considérons le premier:
\begin{equation*}
\fun{insl}(x,\fun{rotl}(t)) \equiv_3 \fun{rotl}(\fun{insl}(x,t)).
\end{equation*}
Implicitement, cette proposition n'a de sens que si l'arbre~\(t\) est
de la forme \(t=\fun{bst}(a, t_1, \fun{bst}(b,t_2,t_3))\) et est un
arbre binaire de recherche, ce qui implique \(b \succ a\). La preuve
est de nature technique, ce qui signifie qu'elle requiert de nombreux
cas et n'apporte pas de nouvelles intuitions, comme le souligne
l'absence d'induction. Nous commençons comme suit:
\begin{equation*}
\begin{array}{@{}r@{\;}l@{\;}l@{}}
  \fun{insl}(x,\fun{rotl}(t))
& = & \fun{insl}(x,\fun{rotl}(\fun{bst}(a, t_1,
\fun{bst}(b,t_2,t_3))))\\
& \xrightarrow{\smash{\zeta}} &
\fun{insl}(x,\fun{bst}(b,\fun{bst}(a,t_1,t_2),t_3)) \quad \otimes
\end{array}
\end{equation*}
Deux cas sont à considérer: \(b \succ x\) ou \(x \succ b\).
\begin{itemize}

  \item Si \(b \succ x\), alors \(\otimes \xrightarrow{\smash{\tau}}
  \fun{bst}(b,\fun{insl}(x,\fun{bst}(a,t_1,t_2)),t_3) \; \otimes\).
  Nous avons donc deux sous-cas: \(a \succ x\) ou \(x \succ a\).

  \(\begin{array}{@{}r@{\;}l@{\;}l@{}}
      \text{\;-- Si \(a \succ x\), alors \(\otimes\)} &
      \xrightarrow{\smash{\tau}} &
      \fun{bst}(b,\fun{bst}(a,\fun{insl}(x,t_1),t_2),t_3)\\
      & \Lla{\zeta} &
      \fun{rotl}(\fun{bst}(a,\fun{insl}(x,t_1),
      \fun{bst}(b,t_2,t_3)))\\
      & \xleftarrow{\smash{\tau}} &
      \fun{rotl}(\fun{insl}(x,\fun{bst}(a,t_1,
      \fun{bst}(b,t_2,t_3))))\\
      & = & \fun{rotl}(\fun{insl}(x,t)).
    \end{array}\)

    \(\begin{array}{@{}r@{\;}l@{\;}l@{}}
      \text{\;-- Si \(x \succ a\), alors \(\otimes\)} &
      \xrightarrow{\smash{\upsilon}} &
      \fun{bst}(b,\fun{bst}(a,t_1,\fun{insl}(x,t_2)),t_3)\\
      & \Lla{\zeta} &
      \fun{rotl}(\fun{bst}(a,t_1,\fun{bst}(b,\fun{insl}(x,t_2),t_3)))\\
      & \xleftarrow{\smash{\tau}} &
      \fun{rotl}(\fun{bst}(a,t_1,\fun{insl}(x,\fun{bst}(b,t_2,t_3))))\\
      & \xleftarrow{\smash{\upsilon}} &
      \fun{rotl}(\fun{insl}(x,\fun{bst}(a,t_1,\fun{bst}(b,t_2,t_3))))\\
      & = & \fun{rotl}(\fun{insl}(x,t)).
      \end{array}\)

  \item Si \(x \succ b\), alors l'hypothèse \(b \succ a\) implique
  \(x \succ a\). Nous avons
  \begin{equation*}
  \begin{array}{@{}r@{\;}l@{\;}l@{\qquad}r@{}}
    \otimes & \xrightarrow{\smash{\upsilon}} &
    \fun{bst}(b,\fun{bst}(a,t_1,t_2),\fun{insl}(x,t_3))\\
    & \Lla{\zeta} &
    \fun{rotl}(\fun{bst}(a,t_1,\fun{bst}(b,t_2,\fun{insl}(x,t_3))))\\
    & \xleftarrow{\smash{\upsilon}} &
    \fun{rotl}(\fun{bst}(a,t_1,\fun{insl}(x,\fun{bst}(b,t_2,t_3))))
    & (x \succ b)\\
    & \xleftarrow{\smash{\upsilon}} &
    \fun{rotl}(\fun{insl}(x,\fun{bst}(a,t_1,\fun{bst}(b,t_2,t_3))))
    & (x \succ a)\\
    & = & \fun{rotl}(\fun{insl}(x,t)). & \hfill\Box
  \end{array}
  \end{equation*}

\end{itemize}

Le lemme restant est \(\fun{insl}(x,\fun{rotr}(t)) \equiv_4
\fun{rotr}(\fun{insl}(x,t))\). Un peu d'algèbre suffit pour montrer
qu'il équivaut à \(\fun{insl}(x,\fun{rotl}(t)) \equiv_3
\fun{rotl}(\fun{insl}(x,t))\).  En effet, nous avons les équations
suivantes équivalentes:
\begin{equation*}
\begin{array}{@{}r@{\;}l@{\;}l@{}}
\fun{insl}(x,\fun{rotl}(t)) & \equiv_3 & \fun{rotl}(\fun{insl}(x,t))\\
\fun{insl}(x,\fun{rotl}(\fun{rotr}(t))) & \equiv &
\fun{rotl}(\fun{insl}(x,\fun{rotr}(t)))\\
\fun{insl}(x,t) & \equiv & \fun{rotl}(\fun{insl}(x,\fun{rotr}(t)))\\
\fun{rotr}(\fun{insl}(x,t)) & \equiv &
\fun{rotr}(\fun{rotl}(\fun{insl}(x,\fun{rotr}(t))))\\
\fun{rotr}(\fun{insl}(x,t)) & \equiv_4 &
\fun{insl}(x,\fun{rotr}(t)).\hfill\Box
\end{array}
\end{equation*}


\mypar{Coût moyen}

Le nombre moyen de comparaisons pour l'insertion d'une racine est le
même que pour l'insertion d'une feuille, parce que les rotations
n'impliquent aucune comparaison:
\begin{equation*}
\OM{\fun{insr}}{n} = \OM{\fun{insr}_0}{n} = \OM{\fun{insl}}{n}
= 2H_n + \frac{2}{n+1} - 3 \sim 2\ln n.
\index{insr@$\OM{\fun{insr}}{n}$}
\index{insr0@$\OM{\fun{insr}_0}{n}$}
\end{equation*}
Ceci dit, les rotations doublent le coût de chaque pas vers le bas
dans l'arbre, et, via les équations~\eqref{eq_Mmem_cmp1}
et~\eqref{eq_Mmem_cmp2} \vpageref{eq_Mmem_cmp2}, nous avons alors
\begin{equation*}
\M{\fun{insr}}{n} = 1 + \frac{2}{n+1}\Expected{E_n} = 1 +
\OM{\fun{mem}}{n{\scriptscriptstyle(-)}} = 4H_n + \frac{4}{n+1} - 3
\sim 4\ln n.
\index{insr@$\M{\fun{insr}}{n}$}
\end{equation*}
Une conséquence du théorème~\eqref{thm_RootLeaf}
\vpageref{thm_RootLeaf} est que toutes les permutations de même taille
donnent naissance au même ensemble d'arbres binaires de recherche, que
ce soit grâce à \fun{mkl/1} ou \fun{mkr/1}. Par conséquent,
l'insertion par \fun{insl/1} ou \fun{insr/1} d'une même clé produira
le même nombre moyen de comparaisons, parce que \(\OM{\fun{insr}}{n} =
\OM{\fun{insr}_0}{n} = \OM{\fun{insl}}{n}\). Par induction sur la
taille, nous concluons que le nombre moyen de comparaisons pour
\fun{mkl/1} et \fun{mkr/1} est le même:
\begin{equation*}
\OM{\fun{mkr}}{n} = \OM{\fun{mkl}}{n} = \Expected{I_n}
= 2(n+1)H_n - 4n.
\index{mkr@$\OM{\fun{mkr}}{n}$}
\end{equation*}
Puisque la seule différence entre \fun{insl/1} et \fun{insr/1} est le
coût supplémentaire d'une rotation par arc descendant, nous réalisons
rapidement, en nous souvenant des équations~\eqref{eq_Mmkl}
et~\eqref{eq_IEn}, que
\begin{equation*}
\M{\fun{mkr}}{n} = \M{\fun{mkl}}{n} + \Expected{I_n}
= n + 2 \cdot \Expected{I_n} + 2 = 4(n+1)H_n - 7n + 2.
\index{mkr@$\M{\fun{mkr}}{n}$}
\end{equation*}

\mypar{Coût amorti}
\index{coût!$\sim$ amorti|(}

Puisque la première phase de l'insertion d'une racine est l'insertion
d'une feuille, les analyses précédentes des extremums du coût de
\fun{insl/2} et \fun{insl\(_1\)/2} sont valables aussi bien pour
\fun{insr/2}. Considérons maintenant les coûts amortis de
\fun{insr/2}, à savoir, les extremums du coût de \fun{mkr/1}.  Soit
\(\OB{\fun{mkr}}{n}\)\index{mkr@$\OB{\fun{mkr}}{n}$} le nombre minimum
de comparaisons pour construire un arbre binaire de recherche de
taille~\(n\) en employant des insertions de racines. Nous avons vu que
le meilleur cas pour l'insertion d'une feuille (\fun{insl/2}) se
produit quand la clé insérée devient l'enfant de la racine. Bien que
cela ne peut conduire au meilleur coût amorti (\fun{mkl/1}), c'est le
meilleur coût amorti quand des insertions de racines (\fun{mkr/1})
sont utilisées, parce que la clé nouvellement insérée devient la
racine avec exactement une rotation (à gauche si la clé était l'enfant
à droite, à droite si elle était l'enfant à gauche), laissant la place
libre à nouveau pour une autre insertion efficace (\fun{insr/2}). Au
bout du compte, l'arbre de recherche est dégénéré; en fait, il y a
exactement deux arbres de coût minimum, dont les formes sont celles de
la \fig~\vref{fig:tree_stack}. Il est intéressant de noter que ces
arbres sont de coût maximum lorsqu'ils sont construits avec des
insertions de feuilles. La première clé n'est pas comparée, donc nous
avons \(\OB{\fun{mkr}}{n} = n - 1 \sim \OB{\fun{mkl}}{n}/\lg n\). Il
est peut-être surprenant que le nombre maximum de comparaisons
\(\OW{\fun{mkr}}{n}\)\index{mkr@$\OW{\fun{mkr}}{n}$} pour faire un
arbre de recherche de taille~\(n\) avec \fun{mkr/1}, c'est-à-dire le
nombre maximum amorti de comparaisons avec \fun{insr/2}, est bien plus
difficile a déterminer que le coût minimum ou moyen.
\cite{GeldenhuysVanderMerwe_2009} montrent que
\begin{equation*}
\abovedisplayskip=4pt
\belowdisplayskip=4pt
\OW{\fun{mkr}}{n} = \frac{1}{4}n^2 + n - 2 - c,
\end{equation*}
où \(c = 0\) si \(n\)~est pair, et \(c=\myfrac{1}/{4}\) si
\(n\)~est impair. Ceci implique
\begin{equation*}
\abovedisplayskip=4pt
\belowdisplayskip=-8pt
\OW{\fun{mkr}}{n} = \frac{1}{2}\OW{\fun{mkl}}{n} + \frac{5}{4}n - 2 -
c \sim \frac{1}{2}\OW{\fun{mkl}}{n}.
\end{equation*}
\index{coût!$\sim$ amorti|)}

\paragraph{Exercices}

\begin{enumerate}

  \item Prouvez \(\fun{bst}_0(t) \equiv \fun{bst}(t)\). Voir les
  définitions de \fun{bst\(_0\)/1}\index{bst0@\fun{bst\(_0\)/1}} et
  \fun{bst/1}\index{bst@\fun{bst/1}}, respectivement, à la
  \fig~\vref{fig:bst0} et à la \fig~\vref{fig:bst}.

  \item Prouvez \(\fun{mem}(y,t) \equiv \fun{mem}_3(y,t)\), c'est-à-dire
  la correction de la recherche à la Andersson. Voir les définitions
  de \fun{mem/2}\index{mem@\fun{mem/2}} et
  \fun{mem\(_3\)/2}\index{mem3@\fun{mem\(_3\)/2}}, à la
  \fig~\vref{fig:mem} et à la \fig~\vref{fig:mem3}.

  \item Prouvez \(\fun{insr}(x,t) \equiv \fun{bst}(x,t_1,t_2)\).
  \index{insr@\fun{insr/2}} En d'autres termes, l'insertion d'une
  racine fait vraiment ce qu'elle dit faire.

  \item Prouvez \(\fun{mklR}(s) \equiv \fun{mkl}(\fun{rev}(s))\).
  \index{mklR@\fun{mklR/1}} \index{mkl@\fun{mkl/1}}
  \index{rev@\fun{rev/1}}

  \item Prouvez \(\fun{bst}(t) \equiv \fun{true}() \Rightarrow
  \fun{mkl}(\fun{pre}(t)) \equiv t\). Voir la définition de
  \fun{pre/1} \index{pre@\fun{pre/1}} à la \fig~\vref{fig:pre}. Est-ce
  que la réciproque est vraie elle aussi?

\end{enumerate}
\index{arbre binaire de recherche!insertion d'une racine|)}

\section{Suppression}
\index{arbre binaire de recherche!suppression|(}

La suppression d'une clé dans un arbre binaire de recherche est un peu
délicate, ce qui contraste avec l'insertion d'une feuille. Bien sûr,
«~suppression~» est une convention dans le contexte des langages
fonctionnels, où les structures de données sont persistantes, donc
suppression signifie que nous reconstruisons un nouvel arbre de
recherche sans la clé en question. Comme avec l'insertion, nous
pourrions simplement commencer avec la recherche de la clé: si elle
est absente, il n'y a rien à faire, sinon nous remplaçons la clé par
son successeur ou prédécesseur immédiat en ordre infixe, c'est-à-dire
par la clé minimum du sous-arbre droit ou la clé maximum du sous-arbre
gauche.

Les définitions pour ces deux phases se trouvent à la
\fig~\vref{fig:del}.
\begin{figure}[!b]
\begin{equation*}
\boxed{
\begin{array}{r@{\;}l@{\;}l}
  \fun{del}(y,\fun{bst}(x,t_1,t_2)) & \rightarrow &
  \fun{bst}(x,\fun{del}(y,t_1),t_2),\; \text{si \(x \succ y\)};\\
\fun{del}(y,\fun{bst}(x,t_1,t_2)) & \rightarrow &
  \fun{bst}(x,t_1,\fun{del}(y,t_2)),\; \text{si \(y \succ x\)};\\
\fun{del}(y,\fun{bst}(x,t_1,t_2)) & \rightarrow &
   \fun{aux}_0(x,t_1,\fun{min}(t_2));\\
\fun{del}(y,\fun{ext}()) & \rightarrow & \fun{ext}().\\
\\
\fun{min}(\fun{bst}(x,\fun{ext}(),t_2)) & \rightarrow & \pair{x}{t_2};\\
\fun{min}(\fun{bst}(x,t_1,t_2)) & \rightarrow &
  \fun{aux}_1(x,\fun{min}(t_1),t_2).\\
\\
\fun{aux}_1(x,\pair{m}{t'_1},t_2) & \rightarrow &
  \pair{m}{\fun{bst}(x,t'_1,t_2)}.\\
\\
\fun{aux}_0(x,t_1,\pair{m}{t'_2}) & \rightarrow & \fun{bst}(m,t_1,t'_2).
\end{array}}
\end{equation*}
\caption{Suppression dans un arbre binaire de recherche\label{fig:del}}
\end{figure}
Nous avons \(\fun{min}(t_2) \twoheadrightarrow
\pair{m}{t'_2}\),\index{min@\fun{min/1}} où \(m\)~est la clé minimum
de l'arbre~\(t_2\) et \(t'_2\)~est la reconstruction de~\(t_2\)
sans~\(m\); en d'autres termes, le n{\oe}ud interne le plus à gauche
de~\(t_2\) contient la clé~\(m\) et ce n{\oe}ud a été remplacé par un
n{\oe}ud externe. L'appel à \fun{aux\(_0\)/3} substitue simplement la
clé~\(x\) à supprimer par son successeur immédiat~\(m\). Le but de la
fonction auxiliaire \fun{aux\(_1\)/3} est de reconstruire l'arbre dans
lequel le minimum a été supprimé. Remarquons que le motif de la
troisième règle n'est pas \(\fun{del}(y,
\fun{bst}(y,t_1,t_2))\),\index{del@\fun{del/2}} parce que nous savons
déjà que \(x=y\) et nous voulons éviter un test d'égalité inutile.

Bien entendu, nous pourrions aussi prendre le maximum du sous-arbre
gauche et cette symétrie arbitraire est en fait ce qui déséquilibre,
en moyenne, les arbres obtenus après des suppressions suivies par au
moins deux insertions. En d'autres termes, ces arbres ne sont pas, en
moyenne, aussi équilibrés que ceux contenant les mêmes clés et
construits seulement par insertions. Ce phénomène est difficile à
comprendre et des exemples sont nécessaire pour le voir à l'{\oe}uvre
\citep{Eppinger_1983,CulbersonMunro_1989,CulbersonEvans_1994,Knuth_1998a,Heyer_2009}.

Une autre sorte d'asymétrie est que la suppression est bien plus
compliquée à programmer que l'insertion. Ce fait a conduit certains
chercheurs à proposer un cadre commun pour l'insertion et la
suppression \citep{Andersson_1991,Hinze_2002}. En particulier, quand
la recherche à la Andersson avec un arbre candidat est modifiée pour
faire une suppression, le programme est plutôt court si le langage de
programmation est impératif.

Une autre approche de la suppression consiste à marquer les n{\oe}uds
visés comme supprimés sans vraiment les supprimer. Ils sont toujours
nécessaires pour de futures comparaisons mais ils ne sont plus
considérés comme faisant partie de la collection de clés que dénote
l'arbre binaire de recherche. En tant que tels, ils sont comme des
zombies, ni vivants ni morts, ou on pourrait parler de suppression
paresseuse. Plus sérieusement, ceci nécessite deux sortes de n{\oe}uds
internes, \fun{bst/3} et \fun{del/3}.\index{del@\fun{del/3}} Cette
autre conception est montrée à la \fig~\vref{fig:del0}.
\begin{figure}
\begin{equation*}
\boxed{
\begin{array}{r@{\;}l@{\;}l}
  \fun{del}_0(y,\fun{bst}(x,t_1,t_2)) & \rightarrow &
  \fun{bst}(x,\fun{del}_0(y,t_1),t_2),\; \text{si \(x \succ y\)};\\
\fun{del}_0(y,\fun{bst}(x,t_1,t_2)) & \rightarrow &
  \fun{bst}(x,t_1,\fun{del}_0(y,t_2)),\; \text{si \(y \succ x\)};\\
\fun{del}_0(y,\fun{bst}(x,t_1,t_2)) & \rightarrow &
  \fun{del}(x,t_1,t_2);\\
\fun{del}_0(y,\fun{del}(x,t_1,t_2)) & \rightarrow &
  \fun{del}(x,\fun{del}_0(y,t_1),t_2),\; \text{si \(x \succ y\)};\\
\fun{del}_0(y,\fun{del}(x,t_1,t_2)) & \rightarrow &
  \fun{del}(x,t_1,\fun{del}_0(y,t_2)),\; \text{si \(y \succ x\)};\\
\fun{del}_0(y,t) & \rightarrow & t.\\
\end{array}}
\end{equation*}
\caption{Suppression paresseuse dans un arbre de recherche\label{fig:del0}}
\end{figure}
Remarquons que l'insertion d'une clé qui a été supprimée
paresseusement n'a pas besoin d'être réalisée à un n{\oe}ud externe:
le constructeur \fun{del/3} est alors simplement changé en
\fun{bst/3}, la marque d'un n{\oe}ud interne normal.

\paragraph{Exercice}

Définissez les insertions usuelles dans ce type d'arbre.
\index{arbre binaire de recherche!suppression|)}

%\section{Merging}
%% Brown & Tarjan Merging

\section{Paramètres moyens}

La hauteur moyenne~\(h_n\) d'un arbre binaire de recherche de
taille~\(n\) a été l'objet d'intenses études \citep{Devroye_1986,
  Devroye_1987, Mahmoud_1992, KnesslSpankowski_2002}, mais les
méthodes, principalement de nature analytiques, sont bien au-delà de
la portée de ce livre. \cite{Reed_2003} a prouvé que
\begin{equation*}
h_n = \alpha \ln n - \frac{3\alpha}{2\alpha - 2} \ln\ln n + \mathcal{O}(1),
\end{equation*}
où \(\alpha\)~est l'unique solution sur l'intervalle \([2,+\infty[\)
de l'équation
\begin{equation*}
\alpha\ln(2e/\alpha) = 1,
\end{equation*}
une valeur approchée étant \(\alpha \simeq 4.31107\), et
\(\mathcal{O}(1)\) désigne une fonction inconnue dont la valeur
absolue est bornée supérieurement par une constante inconnue. La
déduction d'une borne supérieure logarithmique par \cite{Aslam_2001}
est particulièrement intéressante. Il a utilisé un modèle probabiliste
et son résultat a été republié par \cite{CLRS_2009} à la section~12.4.

\cite{ChauvinDrmotaJabbour-Hattab_2001} ont étudié la largueur moyenne
des arbres binaires de recherche.
